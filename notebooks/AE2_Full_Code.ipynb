{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##CS2 Match Prediction System - Step 1: Environment Setup and Data Loading\n",
        "\n",
        "**This script**:\n",
        "1. Installs required packages\n",
        "2. Imports necessary libraries\n",
        "3. Loads and validates the data files\n",
        "4. Performs initial data exploration"
      ],
      "metadata": {
        "id": "i4owjiw1zNmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 1: Package Installation\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 1.1: Installing Required Packages\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package via pip (quiet mode disabled so errors are visible).\"\"\"\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# IMPORTANT:\n",
        "# Do NOT install numpy/pandas on Colab â†’ They are already compatible with Python 3.12\n",
        "packages = [\n",
        "    \"scikit-learn\",\n",
        "    \"lightgbm\",\n",
        "    \"xgboost\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"pyyaml\",\n",
        "    \"joblib\",\n",
        "    \"openpyxl\"\n",
        "]\n",
        "\n",
        "print(\"\\nInstalling required packages (no strict versions)...\")\n",
        "for package in packages:\n",
        "    try:\n",
        "        print(f\"Installing {package}...\", end=\" \")\n",
        "        install_package(package)\n",
        "        print(\"\")\n",
        "    except Exception as e:\n",
        "        print(f\" (Error: {e})\")\n",
        "\n",
        "print(\"\\n Package installation complete!\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: Import Libraries\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1.2: Importing Libraries\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\" Libraries imported successfully!\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: Configuration Setup\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1.3: Setting Up Configuration\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "config = {\n",
        "    'random_seed': 42,\n",
        "    'test_size': 0.2,\n",
        "    'n_folds': 5,\n",
        "    'elo_k_factor': 32,\n",
        "    'elo_default': 1500,\n",
        "    'calibration_bins': 15,\n",
        "    'lgbm_params': {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'binary_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'verbose': -1,\n",
        "        'random_state': 42\n",
        "    },\n",
        "    'xgb_params': {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'max_depth': 6,\n",
        "        'learning_rate': 0.05,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.9,\n",
        "        'random_state': 42\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Configuration created:\")\n",
        "for k, v in config.items():\n",
        "    if isinstance(v, dict):\n",
        "        print(f\"  - {k}: (dict with {len(v)} entries)\")\n",
        "    else:\n",
        "        print(f\"  - {k}: {v}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: File Upload Instructions\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1.4: File Upload\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "FREEZE_TIME_FILE = '/content/freeze_time_features.csv'\n",
        "MATCHES_FILE = '/content/matches.xlsx'\n",
        "\n",
        "print(\"Assuming files will be at /content/... once uploaded.\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: Load Data Files\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1.5: Loading Data Files\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"Loading rounds data from: {FREEZE_TIME_FILE}\")\n",
        "try:\n",
        "    df_rounds = pd.read_csv(FREEZE_TIME_FILE)\n",
        "    print(f\" Loaded {len(df_rounds):,} rows from freeze_time_features.csv\")\n",
        "except Exception as e:\n",
        "    print(f\" Error: {e}\")\n",
        "    df_rounds = None\n",
        "\n",
        "print(f\"\\nLoading matches metadata from: {MATCHES_FILE}\")\n",
        "try:\n",
        "    df_matches = pd.read_excel(MATCHES_FILE)\n",
        "    print(f\" Loaded {len(df_matches):,} rows from matches.xlsx\")\n",
        "except Exception as e:\n",
        "    print(f\" Error: {e}\")\n",
        "    df_matches = None\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: Data Validation and Exploration\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1.6: Data Validation and Exploration\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if df_rounds is not None:\n",
        "    print(\"\\n--- FREEZE TIME FEATURES DATA ---\")\n",
        "    print(df_rounds.head(3))\n",
        "    print(\"\\nShape:\", df_rounds.shape)\n",
        "    print(\"\\nColumns:\", list(df_rounds.columns))\n",
        "\n",
        "    required_features = [\n",
        "        'map','side','is_pistol','is_ot','score_diff','start_cash','loss_bonus',\n",
        "        'consec_losses','equip_value','rifle_cnt','smg_cnt','shotgun_cnt',\n",
        "        'awp_cnt','helmets','kevlar','kits','flash_cnt','smoke_cnt','he_cnt',\n",
        "        'molotov_cnt','timeout_flag','round_win'\n",
        "    ]\n",
        "\n",
        "    missing = [f for f in required_features if f not in df_rounds.columns]\n",
        "    if missing:\n",
        "        print(\"\\n Missing required columns:\", missing)\n",
        "    else:\n",
        "        print(\"\\n All required features present\")\n",
        "\n",
        "if df_matches is not None:\n",
        "    print(\"\\n--- MATCHES METADATA ---\")\n",
        "    print(df_matches.head(5))\n",
        "    print(\"\\nShape:\", df_matches.shape)\n",
        "\n",
        "    required_cols = ['Event Name', 'Time', 'Team 1', 'Team 2', 'Match ID']\n",
        "    missing = [c for c in required_cols if c not in df_matches.columns]\n",
        "    if missing:\n",
        "        print(\"\\n Missing required match columns:\", missing)\n",
        "    else:\n",
        "        print(\"\\n All required match columns present\")\n",
        "\n",
        "    # Convert time column\n",
        "    try:\n",
        "        df_matches['Time'] = pd.to_datetime(df_matches['Time'])\n",
        "        print(\"\\n Converted Time column to datetime\")\n",
        "    except Exception as e:\n",
        "        print(\" Error converting Time:\", e)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: Data Integration Check\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1.7: Data Integration Check\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if df_rounds is not None and df_matches is not None:\n",
        "    r_ids = set(df_rounds['match_id'])\n",
        "    m_ids = set(df_matches['Match ID'])\n",
        "\n",
        "    common = r_ids.intersection(m_ids)\n",
        "\n",
        "    print(f\"Freeze-time match IDs: {len(r_ids)}\")\n",
        "    print(f\"Match metadata IDs:    {len(m_ids)}\")\n",
        "    print(f\"Common match IDs:      {len(common)}\")\n",
        "\n",
        "    if common:\n",
        "        print(\" Files can be linked via Match ID\")\n",
        "    else:\n",
        "        print(\" No common match IDs found\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 8: Summary\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1 COMPLETE: Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        " Environment ready\n",
        " Data loaded\n",
        " Validation completed\n",
        "\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "WYipvxzTzfmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2228a5-f956-400b-a4e0-2d44888c260f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1.1: Installing Required Packages\n",
            "================================================================================\n",
            "\n",
            "Installing required packages (no strict versions)...\n",
            "Installing scikit-learn... \n",
            "Installing lightgbm... \n",
            "Installing xgboost... \n",
            "Installing matplotlib... \n",
            "Installing seaborn... \n",
            "Installing pyyaml... \n",
            "Installing joblib... \n",
            "Installing openpyxl... \n",
            "\n",
            " Package installation complete!\n",
            "\n",
            "================================================================================\n",
            "STEP 1.2: Importing Libraries\n",
            "================================================================================\n",
            " Libraries imported successfully!\n",
            "\n",
            "================================================================================\n",
            "STEP 1.3: Setting Up Configuration\n",
            "================================================================================\n",
            "Configuration created:\n",
            "  - random_seed: 42\n",
            "  - test_size: 0.2\n",
            "  - n_folds: 5\n",
            "  - elo_k_factor: 32\n",
            "  - elo_default: 1500\n",
            "  - calibration_bins: 15\n",
            "  - lgbm_params: (dict with 10 entries)\n",
            "  - xgb_params: (dict with 7 entries)\n",
            "\n",
            "================================================================================\n",
            "STEP 1.4: File Upload\n",
            "================================================================================\n",
            "Assuming files will be at /content/... once uploaded.\n",
            "\n",
            "================================================================================\n",
            "STEP 1.5: Loading Data Files\n",
            "================================================================================\n",
            "Loading rounds data from: /content/freeze_time_features.csv\n",
            " Loaded 10,000 rows from freeze_time_features.csv\n",
            "\n",
            "Loading matches metadata from: /content/matches.xlsx\n",
            " Loaded 101 rows from matches.xlsx\n",
            "\n",
            "================================================================================\n",
            "STEP 1.6: Data Validation and Exploration\n",
            "================================================================================\n",
            "\n",
            "--- FREEZE TIME FEATURES DATA ---\n",
            "   match_id        map  round_num  tick side  team_num     team_name  \\\n",
            "0   2359876  de_mirage          0     1   CT         2  Eternal Fire   \n",
            "1   2359876  de_mirage          1     1    T         3   GamerLegion   \n",
            "2   2359876  de_mirage          2     2   CT         2  Eternal Fire   \n",
            "\n",
            "       opp_name  is_pistol  is_ot  ...  molotov_cnt  opp_rifle_cnt  \\\n",
            "0   GamerLegion          1      0  ...            5              5   \n",
            "1  Eternal Fire          0      0  ...            3              4   \n",
            "2   GamerLegion          0      0  ...            3              4   \n",
            "\n",
            "   opp_smg_cnt  opp_shotgun_cnt  opp_awp_cnt  opp_flash_cnt  opp_smoke_cnt  \\\n",
            "0            0                0            0              5              4   \n",
            "1            1                0            0              0              2   \n",
            "2            0                0            1              0              3   \n",
            "\n",
            "   opp_he_cnt  opp_molotov_cnt  timeout_flag  \n",
            "0           1                3             0  \n",
            "1           2                0             0  \n",
            "2           1                1             0  \n",
            "\n",
            "[3 rows x 36 columns]\n",
            "\n",
            "Shape: (10000, 36)\n",
            "\n",
            "Columns: ['match_id', 'map', 'round_num', 'tick', 'side', 'team_num', 'team_name', 'opp_name', 'is_pistol', 'is_ot', 'score_diff', 'round_win', 'start_cash', 'loss_bonus', 'consec_losses', 'equip_value', 'rifle_cnt', 'smg_cnt', 'shotgun_cnt', 'awp_cnt', 'helmets', 'kevlar', 'kits', 'flash_cnt', 'smoke_cnt', 'he_cnt', 'molotov_cnt', 'opp_rifle_cnt', 'opp_smg_cnt', 'opp_shotgun_cnt', 'opp_awp_cnt', 'opp_flash_cnt', 'opp_smoke_cnt', 'opp_he_cnt', 'opp_molotov_cnt', 'timeout_flag']\n",
            "\n",
            " All required features present\n",
            "\n",
            "--- MATCHES METADATA ---\n",
            "                       Event Name Maps                Time        Team 1  \\\n",
            "0  CS2 Tournament Series - Week 1  bo3 2022-10-01 11:00:00  Eternal Fire   \n",
            "1  CS2 Tournament Series - Week 1  bo3 2022-10-01 18:00:00         Monte   \n",
            "2  CS2 Tournament Series - Week 1  bo3 2022-10-02 01:00:00        Fnatic   \n",
            "3  CS2 Tournament Series - Week 1  bo3 2022-10-02 12:00:00         Fluxo   \n",
            "4  CS2 Tournament Series - Week 1  bo3 2022-10-02 19:00:00     Rare Atom   \n",
            "\n",
            "   Result 1       Team 2  Result 2  Match ID  \n",
            "0        27  GamerLegion        23   2359876  \n",
            "1        34         NAVI        19   2359877  \n",
            "2        34     Astralis        28   2359878  \n",
            "3        35       Spirit        26   2359879  \n",
            "4        32         NAVI        22   2359880  \n",
            "\n",
            "Shape: (101, 8)\n",
            "\n",
            " All required match columns present\n",
            "\n",
            " Converted Time column to datetime\n",
            "\n",
            "================================================================================\n",
            "STEP 1.7: Data Integration Check\n",
            "================================================================================\n",
            "Freeze-time match IDs: 101\n",
            "Match metadata IDs:    101\n",
            "Common match IDs:      101\n",
            " Files can be linked via Match ID\n",
            "\n",
            "================================================================================\n",
            "STEP 1 COMPLETE: Summary\n",
            "================================================================================\n",
            "\n",
            " Environment ready\n",
            " Data loaded\n",
            " Validation completed\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CS2 Match Prediction System - Step 2: Feature Engineering & Elo Rating Construction\n",
        "\n",
        "**This script**:\n",
        "1. Extracts series_type from matches.xlsx\n",
        "2. Implements Elo rating system with event-based freezing\n",
        "3. Calculates pre-event Elo ratings chronologically\n",
        "4. Merges Elo ratings into round-level data\n",
        "5. Validates all features\n",
        "\n",
        "**Prerequisites:** Step 1 completed with synchronized files loaded\n"
      ],
      "metadata": {
        "id": "8S6blXMwzssY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 2: FEATURE ENGINEERING & ELO RATING CONSTRUCTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Ensure data is loaded from Step 1\n",
        "# If running as separate script, load the data:\n",
        "# df_rounds = pd.read_csv('/content/freeze_time_features.csv')\n",
        "# df_matches = pd.read_excel('/content/matches.xlsx')\n",
        "# df_matches['Time'] = pd.to_datetime(df_matches['Time'])\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: Extract Series Type from Matches\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2.1: Extracting Series Type from Matches\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def extract_series_type(maps_str):\n",
        "    \"\"\"\n",
        "    Extract series type from Maps column\n",
        "    Examples: 'bo3' -> 3, 'bo5' -> 5, 'bo1' -> 1\n",
        "    \"\"\"\n",
        "    if pd.isna(maps_str):\n",
        "        return 1\n",
        "\n",
        "    maps_str = str(maps_str).lower().strip()\n",
        "\n",
        "    if 'bo5' in maps_str:\n",
        "        return 5\n",
        "    elif 'bo3' in maps_str:\n",
        "        return 3\n",
        "    elif 'bo2' in maps_str:\n",
        "        return 2\n",
        "    elif 'bo1' in maps_str:\n",
        "        return 1\n",
        "    else:\n",
        "        # Try to extract number\n",
        "        import re\n",
        "        match = re.search(r'bo(\\d+)', maps_str)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "        return 3  # Default to bo3\n",
        "\n",
        "print(\"Extracting series_type from Maps column...\")\n",
        "df_matches['series_type'] = df_matches['Maps'].apply(extract_series_type)\n",
        "\n",
        "print(\"\\nSeries Type Distribution:\")\n",
        "print(df_matches['series_type'].value_counts().sort_index())\n",
        "print(f\"\\n Series type extracted for {len(df_matches)} matches\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: Prepare Match-Level Data for Elo\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2.2: Preparing Match-Level Data for Elo\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create clean match dataframe\n",
        "df_matches_elo = df_matches[['Match ID', 'Time', 'Event Name', 'Team 1', 'Team 2',\n",
        "                              'Result 1', 'Result 2', 'series_type']].copy()\n",
        "df_matches_elo.columns = ['match_id', 'match_time', 'event_name', 'team1', 'team2',\n",
        "                          'score1', 'score2', 'series_type']\n",
        "\n",
        "# Sort by time for chronological Elo calculation\n",
        "df_matches_elo = df_matches_elo.sort_values('match_time').reset_index(drop=True)\n",
        "\n",
        "print(f\" Prepared {len(df_matches_elo)} matches sorted chronologically\")\n",
        "print(f\"  Date range: {df_matches_elo['match_time'].min()} to {df_matches_elo['match_time'].max()}\")\n",
        "print(f\"  Unique teams: {df_matches_elo['team1'].nunique() + df_matches_elo['team2'].nunique()}\")\n",
        "print(f\"  Unique events: {df_matches_elo['event_name'].nunique()}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: Elo Rating System Implementation\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2.3: Implementing Elo Rating System\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "class EloRatingSystem:\n",
        "    \"\"\"\n",
        "    Elo rating system for CS2 teams with event-based freezing\n",
        "    \"\"\"\n",
        "    def __init__(self, k_factor=32, default_rating=1500):\n",
        "        self.k_factor = k_factor\n",
        "        self.default_rating = default_rating\n",
        "        self.ratings = {}  # team -> current rating\n",
        "        self.event_ratings = {}  # (team, event) -> pre-event rating\n",
        "        self.rating_history = []\n",
        "\n",
        "    def get_rating(self, team):\n",
        "        \"\"\"Get current rating for a team\"\"\"\n",
        "        if team not in self.ratings:\n",
        "            self.ratings[team] = self.default_rating\n",
        "        return self.ratings[team]\n",
        "\n",
        "    def expected_score(self, rating_a, rating_b):\n",
        "        \"\"\"Calculate expected score for team A\"\"\"\n",
        "        return 1 / (1 + 10 ** ((rating_b - rating_a) / 400))\n",
        "\n",
        "    def update_ratings(self, team_a, team_b, score_a, score_b, match_id, match_time):\n",
        "        \"\"\"\n",
        "        Update ratings based on match result\n",
        "        Score is rounds won (e.g., 27-23)\n",
        "        \"\"\"\n",
        "        rating_a = self.get_rating(team_a)\n",
        "        rating_b = self.get_rating(team_b)\n",
        "\n",
        "        # Calculate actual score (normalize to 0-1)\n",
        "        total_rounds = score_a + score_b\n",
        "        if total_rounds == 0:\n",
        "            actual_a = 0.5\n",
        "        else:\n",
        "            actual_a = score_a / total_rounds\n",
        "\n",
        "        # Calculate expected scores\n",
        "        expected_a = self.expected_score(rating_a, rating_b)\n",
        "        expected_b = 1 - expected_a\n",
        "\n",
        "        # Update ratings\n",
        "        new_rating_a = rating_a + self.k_factor * (actual_a - expected_a)\n",
        "        new_rating_b = rating_b + self.k_factor * ((1 - actual_a) - expected_b)\n",
        "\n",
        "        # Store history\n",
        "        self.rating_history.append({\n",
        "            'match_id': match_id,\n",
        "            'match_time': match_time,\n",
        "            'team_a': team_a,\n",
        "            'team_b': team_b,\n",
        "            'rating_a_before': rating_a,\n",
        "            'rating_b_before': rating_b,\n",
        "            'rating_a_after': new_rating_a,\n",
        "            'rating_b_after': new_rating_b,\n",
        "            'score_a': score_a,\n",
        "            'score_b': score_b\n",
        "        })\n",
        "\n",
        "        # Update current ratings\n",
        "        self.ratings[team_a] = new_rating_a\n",
        "        self.ratings[team_b] = new_rating_b\n",
        "\n",
        "        return new_rating_a, new_rating_b\n",
        "\n",
        "    def freeze_rating_for_event(self, team, event):\n",
        "        \"\"\"Freeze team's rating at the start of an event\"\"\"\n",
        "        key = (team, event)\n",
        "        if key not in self.event_ratings:\n",
        "            self.event_ratings[key] = self.get_rating(team)\n",
        "        return self.event_ratings[key]\n",
        "\n",
        "    def get_pre_event_rating(self, team, event):\n",
        "        \"\"\"\n",
        "        Get the rating a team had before an event started\n",
        "        Returns (rating, is_missing_flag)\n",
        "        \"\"\"\n",
        "        key = (team, event)\n",
        "\n",
        "        # Check if we have a frozen rating for this team-event combo\n",
        "        if key in self.event_ratings:\n",
        "            # Check if team had any history before this event\n",
        "            has_history = len([h for h in self.rating_history if h['team_a'] == team or h['team_b'] == team]) > 0\n",
        "            return self.event_ratings[key], 0 if has_history else 1\n",
        "\n",
        "        # If not frozen yet, freeze it now\n",
        "        rating = self.get_rating(team)\n",
        "        has_history = len([h for h in self.rating_history if h['team_a'] == team or h['team_b'] == team]) > 0\n",
        "\n",
        "        self.event_ratings[key] = rating\n",
        "        return rating, 0 if has_history else 1\n",
        "\n",
        "# Initialize Elo system\n",
        "print(\"Initializing Elo rating system...\")\n",
        "print(f\"  K-factor: 32\")\n",
        "print(f\"  Default rating: 1500\")\n",
        "\n",
        "elo_system = EloRatingSystem(k_factor=32, default_rating=1500)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: Calculate Elo Ratings Chronologically\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2.4: Calculating Elo Ratings Chronologically\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Track events we've seen\n",
        "processed_events = set()\n",
        "match_elo_data = []\n",
        "\n",
        "print(f\"\\nProcessing {len(df_matches_elo)} matches chronologically...\\n\")\n",
        "\n",
        "for idx, row in df_matches_elo.iterrows():\n",
        "    match_id = row['match_id']\n",
        "    event = row['event_name']\n",
        "    team1 = row['team1']\n",
        "    team2 = row['team2']\n",
        "    score1 = row['score1']\n",
        "    score2 = row['score2']\n",
        "    match_time = row['match_time']\n",
        "\n",
        "    # Freeze ratings for this event if first time seeing it\n",
        "    if event not in processed_events:\n",
        "        # Get all teams in this event\n",
        "        event_matches = df_matches_elo[df_matches_elo['event_name'] == event]\n",
        "        event_teams = set(event_matches['team1'].tolist() + event_matches['team2'].tolist())\n",
        "\n",
        "        # Freeze ratings for all teams in this event\n",
        "        for team in event_teams:\n",
        "            elo_system.freeze_rating_for_event(team, event)\n",
        "\n",
        "        processed_events.add(event)\n",
        "        print(f\"  Event {len(processed_events)}: '{event}' - Froze ratings for {len(event_teams)} teams\")\n",
        "\n",
        "    # Get pre-event ratings for both teams\n",
        "    team1_elo, team1_missing = elo_system.get_pre_event_rating(team1, event)\n",
        "    team2_elo, team2_missing = elo_system.get_pre_event_rating(team2, event)\n",
        "\n",
        "    # Store match-level Elo data\n",
        "    match_elo_data.append({\n",
        "        'match_id': match_id,\n",
        "        'event_name': event,\n",
        "        'team1': team1,\n",
        "        'team2': team2,\n",
        "        'team1_elo_pre_event': team1_elo,\n",
        "        'team2_elo_pre_event': team2_elo,\n",
        "        'team1_elo_missing': team1_missing,\n",
        "        'team2_elo_missing': team2_missing,\n",
        "        'elo_diff_team1': team1_elo - team2_elo\n",
        "    })\n",
        "\n",
        "    # Update ratings after the match (for future matches)\n",
        "    elo_system.update_ratings(team1, team2, score1, score2, match_id, match_time)\n",
        "\n",
        "print(f\"\\n Processed all matches and calculated Elo ratings\")\n",
        "print(f\" Tracked {len(processed_events)} unique events\")\n",
        "print(f\" Rated {len(elo_system.ratings)} unique teams\")\n",
        "\n",
        "# Create match Elo dataframe\n",
        "df_match_elo = pd.DataFrame(match_elo_data)\n",
        "\n",
        "print(\"\\nElo Rating Summary:\")\n",
        "ratings_list = list(elo_system.ratings.values())\n",
        "print(f\"  Mean rating: {np.mean(ratings_list):.1f}\")\n",
        "print(f\"  Std rating: {np.std(ratings_list):.1f}\")\n",
        "print(f\"  Min rating: {np.min(ratings_list):.1f}\")\n",
        "print(f\"  Max rating: {np.max(ratings_list):.1f}\")\n",
        "\n",
        "# Show teams with missing ratings at start\n",
        "missing_count = df_match_elo['team1_elo_missing'].sum() + df_match_elo['team2_elo_missing'].sum()\n",
        "total_team_appearances = len(df_match_elo) * 2\n",
        "print(f\"\\nTeams with no prior history: {missing_count}/{total_team_appearances} ({missing_count/total_team_appearances*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nTop 10 teams by final Elo:\")\n",
        "top_teams = sorted(elo_system.ratings.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "for i, (team, rating) in enumerate(top_teams, 1):\n",
        "    print(f\"  {i:2d}. {team:30s} {rating:.1f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: Merge Elo Ratings into Round Data\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2.5: Merging Elo Ratings into Round Data\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nOriginal rounds data: {len(df_rounds):,} rounds\")\n",
        "\n",
        "def merge_elo_to_rounds(df_rounds, df_match_elo):\n",
        "    \"\"\"\n",
        "    Merge Elo ratings into rounds dataframe\n",
        "    \"\"\"\n",
        "    df_rounds_with_elo = df_rounds.copy()\n",
        "\n",
        "    # Initialize Elo columns\n",
        "    df_rounds_with_elo['team_elo_pre_event'] = np.nan\n",
        "    df_rounds_with_elo['opp_elo_pre_event'] = np.nan\n",
        "    df_rounds_with_elo['elo_diff'] = np.nan\n",
        "    df_rounds_with_elo['elo_missing'] = 0\n",
        "\n",
        "    # Create lookup dictionary for faster access\n",
        "    elo_lookup = {}\n",
        "    for _, row in df_match_elo.iterrows():\n",
        "        elo_lookup[row['match_id']] = row\n",
        "\n",
        "    print(\"Merging Elo ratings into rounds...\")\n",
        "    merge_count = 0\n",
        "\n",
        "    for idx in df_rounds_with_elo.index:\n",
        "        match_id = df_rounds_with_elo.loc[idx, 'match_id']\n",
        "        team_name = df_rounds_with_elo.loc[idx, 'team_name']\n",
        "\n",
        "        if match_id not in elo_lookup:\n",
        "            continue\n",
        "\n",
        "        match_elo = elo_lookup[match_id]\n",
        "\n",
        "        # Determine if this team is team1 or team2\n",
        "        if team_name == match_elo['team1']:\n",
        "            df_rounds_with_elo.loc[idx, 'team_elo_pre_event'] = match_elo['team1_elo_pre_event']\n",
        "            df_rounds_with_elo.loc[idx, 'opp_elo_pre_event'] = match_elo['team2_elo_pre_event']\n",
        "            df_rounds_with_elo.loc[idx, 'elo_diff'] = match_elo['elo_diff_team1']\n",
        "            df_rounds_with_elo.loc[idx, 'elo_missing'] = match_elo['team1_elo_missing']\n",
        "            merge_count += 1\n",
        "        elif team_name == match_elo['team2']:\n",
        "            df_rounds_with_elo.loc[idx, 'team_elo_pre_event'] = match_elo['team2_elo_pre_event']\n",
        "            df_rounds_with_elo.loc[idx, 'opp_elo_pre_event'] = match_elo['team1_elo_pre_event']\n",
        "            df_rounds_with_elo.loc[idx, 'elo_diff'] = -match_elo['elo_diff_team1']\n",
        "            df_rounds_with_elo.loc[idx, 'elo_missing'] = match_elo['team2_elo_missing']\n",
        "            merge_count += 1\n",
        "\n",
        "    print(f\"   Merged Elo for {merge_count:,} rounds\")\n",
        "    return df_rounds_with_elo\n",
        "\n",
        "df_rounds_with_elo = merge_elo_to_rounds(df_rounds, df_match_elo)\n",
        "\n",
        "# Check merge success\n",
        "elo_populated = df_rounds_with_elo['team_elo_pre_event'].notna().sum()\n",
        "print(f\"\\n Elo ratings merged for {elo_populated:,}/{len(df_rounds_with_elo):,} rounds ({elo_populated/len(df_rounds_with_elo)*100:.1f}%)\")\n",
        "\n",
        "if elo_populated < len(df_rounds_with_elo):\n",
        "    print(f\"  {len(df_rounds_with_elo) - elo_populated:,} rounds missing Elo (should not happen with synchronized files)\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: Add Series Type and Event to Rounds\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2.6: Adding Series Type and Event to Rounds\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Merge series_type from matches\n",
        "df_matches_meta = df_matches[['Match ID', 'series_type', 'Event Name']].copy()\n",
        "df_matches_meta.columns = ['match_id', 'series_type', 'event_name']\n",
        "\n",
        "df_rounds_final = df_rounds_with_elo.merge(df_matches_meta, on='match_id', how='left')\n",
        "\n",
        "print(f\" Series type and event added to all rounds\")\n",
        "print(f\"\\nSeries type distribution in rounds:\")\n",
        "print(df_rounds_final['series_type'].value_counts().sort_index())\n",
        "\n",
        "print(f\"\\nEvent distribution in rounds:\")\n",
        "print(df_rounds_final['event_name'].value_counts())\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: Feature Summary and Validation\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2.7: Feature Summary and Validation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# List all features\n",
        "print(\"\\nFinal Feature List:\")\n",
        "feature_categories = {\n",
        "    'Match Context': ['map', 'side', 'series_type', 'is_pistol', 'is_ot', 'event_name'],\n",
        "    'Score & Economy': ['score_diff', 'start_cash', 'loss_bonus', 'consec_losses', 'equip_value'],\n",
        "    'Weapons': ['rifle_cnt', 'smg_cnt', 'shotgun_cnt', 'awp_cnt'],\n",
        "    'Armor & Utility': ['helmets', 'kevlar', 'kits'],\n",
        "    'Grenades': ['flash_cnt', 'smoke_cnt', 'he_cnt', 'molotov_cnt'],\n",
        "    'Opponent Info': ['opp_rifle_cnt', 'opp_smg_cnt', 'opp_shotgun_cnt', 'opp_awp_cnt',\n",
        "                      'opp_flash_cnt', 'opp_smoke_cnt', 'opp_he_cnt', 'opp_molotov_cnt'],\n",
        "    'Other': ['timeout_flag'],\n",
        "    'Elo Ratings': ['team_elo_pre_event', 'opp_elo_pre_event', 'elo_diff', 'elo_missing'],\n",
        "    'Target': ['round_win']\n",
        "}\n",
        "\n",
        "total_features = 0\n",
        "for category, features in feature_categories.items():\n",
        "    present = [f for f in features if f in df_rounds_final.columns]\n",
        "    total_features += len(present)\n",
        "    print(f\"\\n{category} ({len(present)}/{len(features)}):\")\n",
        "    for f in present:\n",
        "        print(f\"   {f}\")\n",
        "    missing = [f for f in features if f not in df_rounds_final.columns]\n",
        "    if missing:\n",
        "        for f in missing:\n",
        "            print(f\"   {f} (MISSING)\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Total features available: {total_features}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing Values Check:\")\n",
        "missing_summary = df_rounds_final.isnull().sum()\n",
        "missing_summary = missing_summary[missing_summary > 0]\n",
        "if len(missing_summary) > 0:\n",
        "    print(missing_summary)\n",
        "else:\n",
        "    print(\"   No missing values!\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 8: Save Processed Data\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2.8: Saving Processed Data\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = '/content/rounds_with_elo.csv'\n",
        "df_rounds_final.to_csv(output_file, index=False)\n",
        "print(f\" Saved processed data to: {output_file}\")\n",
        "print(f\"  Shape: {df_rounds_final.shape}\")\n",
        "\n",
        "# Save match-level Elo\n",
        "match_elo_file = '/content/match_elo.csv'\n",
        "df_match_elo.to_csv(match_elo_file, index=False)\n",
        "print(f\" Saved match Elo data to: {match_elo_file}\")\n",
        "\n",
        "# Save Elo history\n",
        "elo_history_file = '/content/elo_history.csv'\n",
        "df_elo_history = pd.DataFrame(elo_system.rating_history)\n",
        "df_elo_history.to_csv(elo_history_file, index=False)\n",
        "print(f\" Saved Elo history to: {elo_history_file}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 9: Data Quality Report\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2.9: Data Quality Report\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nDataset Overview:\")\n",
        "print(f\"  Total rounds: {len(df_rounds_final):,}\")\n",
        "print(f\"  Total matches: {df_rounds_final['match_id'].nunique()}\")\n",
        "print(f\"  Total teams: {df_rounds_final['team_name'].nunique()}\")\n",
        "print(f\"  Total maps: {df_rounds_final['map'].nunique()}\")\n",
        "print(f\"  Total events: {df_rounds_final['event_name'].nunique()}\")\n",
        "\n",
        "print(f\"\\nElo Statistics:\")\n",
        "print(f\"  Mean team Elo: {df_rounds_final['team_elo_pre_event'].mean():.1f}\")\n",
        "print(f\"  Std team Elo: {df_rounds_final['team_elo_pre_event'].std():.1f}\")\n",
        "print(f\"  Min team Elo: {df_rounds_final['team_elo_pre_event'].min():.1f}\")\n",
        "print(f\"  Max team Elo: {df_rounds_final['team_elo_pre_event'].max():.1f}\")\n",
        "print(f\"  Mean Elo diff: {df_rounds_final['elo_diff'].mean():.1f}\")\n",
        "print(f\"  Std Elo diff: {df_rounds_final['elo_diff'].std():.1f}\")\n",
        "print(f\"  Rounds with missing Elo flag: {df_rounds_final['elo_missing'].sum()} ({df_rounds_final['elo_missing'].mean()*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nTarget Distribution:\")\n",
        "print(df_rounds_final['round_win'].value_counts(normalize=True))\n",
        "\n",
        "print(f\"\\nMap Distribution:\")\n",
        "print(df_rounds_final['map'].value_counts())\n",
        "\n",
        "print(f\"\\nSide Distribution:\")\n",
        "print(df_rounds_final['side'].value_counts())\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2 COMPLETE: Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        " Feature engineering completed successfully!\n",
        "\n",
        "Accomplishment:\n",
        "----------------------\n",
        "1. Extracted series_type from matches.xlsx\n",
        "2. Implemented Elo rating system with event-based freezing\n",
        "3. Calculated pre-event Elo ratings for all teams chronologically\n",
        "4. Successfully merged Elo ratings into round-level data (100% coverage)\n",
        "5. Validated all required features are present\n",
        "6. Saved processed dataset for model training\n",
        "\n",
        "Key Statistics:\n",
        "---------------\n",
        "\"\"\")\n",
        "print(f\"  - Total rounds: {len(df_rounds_final):,}\")\n",
        "print(f\"  - Rounds with Elo: {(df_rounds_final['team_elo_pre_event'].notna()).sum():,}\")\n",
        "print(f\"  - Unique teams rated: {len(elo_system.ratings)}\")\n",
        "print(f\"  - Elo rating range: {df_rounds_final['team_elo_pre_event'].min():.0f} - {df_rounds_final['team_elo_pre_event'].max():.0f}\")\n",
        "print(f\"  - Features available: {total_features}\")\n",
        "\n",
        "print(\"\"\"\n",
        "Files saved:\n",
        "------------\n",
        "  - rounds_with_elo.csv (main dataset with all features)\n",
        "  - match_elo.csv (match-level Elo ratings)\n",
        "  - elo_history.csv (rating evolution over time)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"END OF STEP 2\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sVCYllmoK25",
        "outputId": "3ade5fc9-5a36-448d-d099-4a77c9cd9398"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 2: FEATURE ENGINEERING & ELO RATING CONSTRUCTION\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 2.1: Extracting Series Type from Matches\n",
            "================================================================================\n",
            "Extracting series_type from Maps column...\n",
            "\n",
            "Series Type Distribution:\n",
            "series_type\n",
            "3    101\n",
            "Name: count, dtype: int64\n",
            "\n",
            " Series type extracted for 101 matches\n",
            "\n",
            "================================================================================\n",
            "STEP 2.2: Preparing Match-Level Data for Elo\n",
            "================================================================================\n",
            " Prepared 101 matches sorted chronologically\n",
            "  Date range: 2022-10-01 11:00:00 to 2022-11-03 17:00:00\n",
            "  Unique teams: 64\n",
            "  Unique events: 7\n",
            "\n",
            "================================================================================\n",
            "STEP 2.3: Implementing Elo Rating System\n",
            "================================================================================\n",
            "Initializing Elo rating system...\n",
            "  K-factor: 32\n",
            "  Default rating: 1500\n",
            "\n",
            "================================================================================\n",
            "STEP 2.4: Calculating Elo Ratings Chronologically\n",
            "================================================================================\n",
            "\n",
            "Processing 101 matches chronologically...\n",
            "\n",
            "  Event 1: 'CS2 Tournament Series - Week 1' - Froze ratings for 19 teams\n",
            "  Event 2: 'CS2 Tournament Series - Week 2' - Froze ratings for 21 teams\n",
            "  Event 3: 'CS2 Tournament Series - Week 3' - Froze ratings for 24 teams\n",
            "  Event 4: 'CS2 Tournament Series - Week 4' - Froze ratings for 18 teams\n",
            "  Event 5: 'CS2 Tournament Series - Week 5' - Froze ratings for 20 teams\n",
            "  Event 6: 'CS2 Tournament Series - Week 6' - Froze ratings for 16 teams\n",
            "  Event 7: 'CS2 Tournament Series - Finals' - Froze ratings for 22 teams\n",
            "\n",
            " Processed all matches and calculated Elo ratings\n",
            " Tracked 7 unique events\n",
            " Rated 34 unique teams\n",
            "\n",
            "Elo Rating Summary:\n",
            "  Mean rating: 1500.0\n",
            "  Std rating: 6.8\n",
            "  Min rating: 1487.8\n",
            "  Max rating: 1517.9\n",
            "\n",
            "Teams with no prior history: 34/202 (16.8%)\n",
            "\n",
            "Top 10 teams by final Elo:\n",
            "   1. SAW                            1517.9\n",
            "   2. Eternal Fire                   1515.3\n",
            "   3. BESTIA                         1512.7\n",
            "   4. Astralis                       1510.6\n",
            "   5. NIP                            1507.4\n",
            "   6. Rare Atom                      1506.1\n",
            "   7. Cloud9                         1505.5\n",
            "   8. BIG                            1504.8\n",
            "   9. TheMongolz                     1503.2\n",
            "  10. TYLOO                          1503.0\n",
            "\n",
            "================================================================================\n",
            "STEP 2.5: Merging Elo Ratings into Round Data\n",
            "================================================================================\n",
            "\n",
            "Original rounds data: 10,000 rounds\n",
            "Merging Elo ratings into rounds...\n",
            "   Merged Elo for 10,000 rounds\n",
            "\n",
            " Elo ratings merged for 10,000/10,000 rounds (100.0%)\n",
            "\n",
            "================================================================================\n",
            "STEP 2.6: Adding Series Type and Event to Rounds\n",
            "================================================================================\n",
            " Series type and event added to all rounds\n",
            "\n",
            "Series type distribution in rounds:\n",
            "series_type\n",
            "3    10000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Event distribution in rounds:\n",
            "event_name\n",
            "CS2 Tournament Series - Finals    1692\n",
            "CS2 Tournament Series - Week 3    1400\n",
            "CS2 Tournament Series - Week 1    1400\n",
            "CS2 Tournament Series - Week 5    1400\n",
            "CS2 Tournament Series - Week 4    1400\n",
            "CS2 Tournament Series - Week 6    1400\n",
            "CS2 Tournament Series - Week 2    1308\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "STEP 2.7: Feature Summary and Validation\n",
            "================================================================================\n",
            "\n",
            "Final Feature List:\n",
            "\n",
            "Match Context (6/6):\n",
            "   map\n",
            "   side\n",
            "   series_type\n",
            "   is_pistol\n",
            "   is_ot\n",
            "   event_name\n",
            "\n",
            "Score & Economy (5/5):\n",
            "   score_diff\n",
            "   start_cash\n",
            "   loss_bonus\n",
            "   consec_losses\n",
            "   equip_value\n",
            "\n",
            "Weapons (4/4):\n",
            "   rifle_cnt\n",
            "   smg_cnt\n",
            "   shotgun_cnt\n",
            "   awp_cnt\n",
            "\n",
            "Armor & Utility (3/3):\n",
            "   helmets\n",
            "   kevlar\n",
            "   kits\n",
            "\n",
            "Grenades (4/4):\n",
            "   flash_cnt\n",
            "   smoke_cnt\n",
            "   he_cnt\n",
            "   molotov_cnt\n",
            "\n",
            "Opponent Info (8/8):\n",
            "   opp_rifle_cnt\n",
            "   opp_smg_cnt\n",
            "   opp_shotgun_cnt\n",
            "   opp_awp_cnt\n",
            "   opp_flash_cnt\n",
            "   opp_smoke_cnt\n",
            "   opp_he_cnt\n",
            "   opp_molotov_cnt\n",
            "\n",
            "Other (1/1):\n",
            "   timeout_flag\n",
            "\n",
            "Elo Ratings (4/4):\n",
            "   team_elo_pre_event\n",
            "   opp_elo_pre_event\n",
            "   elo_diff\n",
            "   elo_missing\n",
            "\n",
            "Target (1/1):\n",
            "   round_win\n",
            "\n",
            "================================================================================\n",
            "Total features available: 36\n",
            "\n",
            "Missing Values Check:\n",
            "   No missing values!\n",
            "\n",
            "================================================================================\n",
            "STEP 2.8: Saving Processed Data\n",
            "================================================================================\n",
            " Saved processed data to: /content/rounds_with_elo.csv\n",
            "  Shape: (10000, 42)\n",
            " Saved match Elo data to: /content/match_elo.csv\n",
            " Saved Elo history to: /content/elo_history.csv\n",
            "\n",
            "================================================================================\n",
            "STEP 2.9: Data Quality Report\n",
            "================================================================================\n",
            "\n",
            "Dataset Overview:\n",
            "  Total rounds: 10,000\n",
            "  Total matches: 101\n",
            "  Total teams: 34\n",
            "  Total maps: 7\n",
            "  Total events: 7\n",
            "\n",
            "Elo Statistics:\n",
            "  Mean team Elo: 1500.8\n",
            "  Std team Elo: 4.5\n",
            "  Min team Elo: 1486.7\n",
            "  Max team Elo: 1513.0\n",
            "  Mean Elo diff: 0.0\n",
            "  Std Elo diff: 6.3\n",
            "  Rounds with missing Elo flag: 1671 (16.7%)\n",
            "\n",
            "Target Distribution:\n",
            "round_win\n",
            "1    0.5625\n",
            "0    0.4375\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Map Distribution:\n",
            "map\n",
            "de_mirage      2066\n",
            "de_dust2       1900\n",
            "de_ancient     1500\n",
            "de_nuke        1400\n",
            "de_inferno     1392\n",
            "de_overpass     942\n",
            "de_vertigo      800\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Side Distribution:\n",
            "side\n",
            "CT    5000\n",
            "T     5000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "STEP 2 COMPLETE: Summary\n",
            "================================================================================\n",
            "\n",
            " Feature engineering completed successfully!\n",
            "\n",
            "Accomplishment:\n",
            "----------------------\n",
            "1. Extracted series_type from matches.xlsx\n",
            "2. Implemented Elo rating system with event-based freezing\n",
            "3. Calculated pre-event Elo ratings for all teams chronologically\n",
            "4. Successfully merged Elo ratings into round-level data (100% coverage)\n",
            "5. Validated all required features are present\n",
            "6. Saved processed dataset for model training\n",
            "\n",
            "Key Statistics:\n",
            "---------------\n",
            "\n",
            "  - Total rounds: 10,000\n",
            "  - Rounds with Elo: 10,000\n",
            "  - Unique teams rated: 34\n",
            "  - Elo rating range: 1487 - 1513\n",
            "  - Features available: 36\n",
            "\n",
            "Files saved:\n",
            "------------\n",
            "  - rounds_with_elo.csv (main dataset with all features)\n",
            "  - match_elo.csv (match-level Elo ratings)\n",
            "  - elo_history.csv (rating evolution over time)\n",
            "\n",
            "\n",
            "================================================================================\n",
            "END OF STEP 2\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CS2 Match Prediction System - Step 3: Data Preparation & Train/Test Split**\n",
        "\n",
        "**This script:**\n",
        "1. Loads processed data from Step 2\n",
        "2. Encodes categorical features\n",
        "3. Creates event-based train/test splits\n",
        "4. Prepares feature matrices for modeling\n",
        "5. Validates data quality\n",
        "\n",
        "**Prerequisites:** Step 2 completed with rounds_with_elo.csv saved"
      ],
      "metadata": {
        "id": "6SYi7U-W0Mg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 3: DATA PREPARATION & TRAIN/TEST SPLIT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: Load Processed Data\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3.1: Loading Processed Data from Step 2\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df_rounds = pd.read_csv('/content/rounds_with_elo.csv')\n",
        "print(f\" Loaded rounds_with_elo.csv: {len(df_rounds):,} rows\")\n",
        "print(f\"  Columns: {len(df_rounds.columns)}\")\n",
        "print(f\"  Matches: {df_rounds['match_id'].nunique()}\")\n",
        "print(f\"  Events: {df_rounds['event_name'].nunique()}\")\n",
        "\n",
        "# Verify Elo columns are present\n",
        "elo_cols = ['team_elo_pre_event', 'opp_elo_pre_event', 'elo_diff', 'elo_missing']\n",
        "missing_elo = [col for col in elo_cols if col not in df_rounds.columns]\n",
        "if missing_elo:\n",
        "    print(f\"   Missing Elo columns: {missing_elo}\")\n",
        "else:\n",
        "    print(f\"   All Elo columns present\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: Define Feature Sets\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3.2: Defining Feature Sets\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Base freeze-time features (no Elo)\n",
        "freeze_time_features = [\n",
        "    # Context (exclude map and side - will be encoded)\n",
        "    'series_type', 'is_pistol', 'is_ot',\n",
        "    # Economy & Score\n",
        "    'score_diff', 'start_cash', 'loss_bonus', 'consec_losses', 'equip_value',\n",
        "    # Weapons\n",
        "    'rifle_cnt', 'smg_cnt', 'shotgun_cnt', 'awp_cnt',\n",
        "    # Armor & Utility\n",
        "    'helmets', 'kevlar', 'kits',\n",
        "    # Grenades\n",
        "    'flash_cnt', 'smoke_cnt', 'he_cnt', 'molotov_cnt',\n",
        "    # Opponent info\n",
        "    'opp_rifle_cnt', 'opp_smg_cnt', 'opp_shotgun_cnt', 'opp_awp_cnt',\n",
        "    'opp_flash_cnt', 'opp_smoke_cnt', 'opp_he_cnt', 'opp_molotov_cnt',\n",
        "    # Other\n",
        "    'timeout_flag'\n",
        "]\n",
        "\n",
        "# Elo features\n",
        "elo_features = [\n",
        "    'team_elo_pre_event', 'opp_elo_pre_event', 'elo_diff', 'elo_missing'\n",
        "]\n",
        "\n",
        "# Categorical features for encoding\n",
        "categorical_features = ['map', 'side']\n",
        "\n",
        "# Target\n",
        "target = 'round_win'\n",
        "\n",
        "print(f\"Feature Sets Defined:\")\n",
        "print(f\"  Freeze-time features: {len(freeze_time_features)}\")\n",
        "print(f\"  Elo features: {len(elo_features)}\")\n",
        "print(f\"  Categorical features: {len(categorical_features)}\")\n",
        "print(f\"  Total numeric features: {len(freeze_time_features) + len(elo_features)}\")\n",
        "\n",
        "# Verify all features exist\n",
        "all_numeric_features = freeze_time_features + elo_features\n",
        "missing_features = [f for f in all_numeric_features if f not in df_rounds.columns]\n",
        "missing_cat = [f for f in categorical_features if f not in df_rounds.columns]\n",
        "\n",
        "if missing_features or missing_cat:\n",
        "    print(f\"\\n Missing features: {missing_features + missing_cat}\")\n",
        "else:\n",
        "    print(f\"\\n All features present in dataset\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: Encode Categorical Features\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3.3: Encoding Categorical Features\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df_encoded = df_rounds.copy()\n",
        "\n",
        "print(\"\\nEncoding categorical features using one-hot encoding...\")\n",
        "\n",
        "# Store mapping for later use\n",
        "categorical_mappings = {}\n",
        "\n",
        "for cat_feat in categorical_features:\n",
        "    print(f\"\\n  Encoding '{cat_feat}':\")\n",
        "    print(f\"    Unique values: {df_encoded[cat_feat].nunique()}\")\n",
        "    print(f\"    Values: {sorted(df_encoded[cat_feat].unique())}\")\n",
        "\n",
        "    # One-hot encode with drop_first=True to avoid multicollinearity\n",
        "    dummies = pd.get_dummies(df_encoded[cat_feat], prefix=cat_feat, drop_first=True)\n",
        "\n",
        "    print(f\"    Created {len(dummies.columns)} dummy variables\")\n",
        "\n",
        "    # Add to dataframe\n",
        "    df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
        "\n",
        "    # Store mapping\n",
        "    categorical_mappings[cat_feat] = {\n",
        "        'original_values': sorted(df_encoded[cat_feat].unique().tolist()),\n",
        "        'dummy_columns': dummies.columns.tolist()\n",
        "    }\n",
        "\n",
        "print(f\"\\n Categorical encoding complete\")\n",
        "print(f\"Dataset shape: {df_rounds.shape} â†’ {df_encoded.shape}\")\n",
        "\n",
        "# Create final feature list (numeric + encoded categoricals)\n",
        "encoded_categorical_features = []\n",
        "for cat_feat in categorical_features:\n",
        "    encoded_categorical_features.extend(categorical_mappings[cat_feat]['dummy_columns'])\n",
        "\n",
        "# All features for models\n",
        "features_freeze_only = freeze_time_features + encoded_categorical_features\n",
        "features_with_elo = freeze_time_features + encoded_categorical_features + elo_features\n",
        "\n",
        "print(f\"\\nFinal Feature Counts:\")\n",
        "print(f\"  Freeze-time only: {len(features_freeze_only)}\")\n",
        "print(f\"  With Elo: {len(features_with_elo)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: Create Event-Based Groups for Cross-Validation\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3.4: Creating Event-Based Groups for Cross-Validation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create event group IDs for CV\n",
        "event_to_id = {event: i for i, event in enumerate(sorted(df_encoded['event_name'].unique()))}\n",
        "df_encoded['event_group'] = df_encoded['event_name'].map(event_to_id)\n",
        "\n",
        "print(f\" Created {df_encoded['event_group'].nunique()} event groups\")\n",
        "print(f\"\\nEvent Group Mapping:\")\n",
        "for event, group_id in sorted(event_to_id.items(), key=lambda x: x[1]):\n",
        "    count = (df_encoded['event_group'] == group_id).sum()\n",
        "    print(f\"  Group {group_id}: {event:40s} ({count:,} rounds)\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: Train/Test Split (Event-Based)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3.5: Creating Train/Test Split (Event-Based)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use last 2 event groups as test set (~20-25% of data)\n",
        "test_event_groups = [5, 6]  # Last 2 events\n",
        "train_mask = ~df_encoded['event_group'].isin(test_event_groups)\n",
        "\n",
        "df_train = df_encoded[train_mask].copy()\n",
        "df_test = df_encoded[~train_mask].copy()\n",
        "\n",
        "print(f\"\\nTrain/Test Split:\")\n",
        "print(f\"  Train set: {len(df_train):,} rounds ({len(df_train)/len(df_encoded)*100:.1f}%)\")\n",
        "print(f\"  Test set: {len(df_test):,} rounds ({len(df_test)/len(df_encoded)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nTrain set:\")\n",
        "print(f\"  Matches: {df_train['match_id'].nunique()}\")\n",
        "print(f\"  Events: {df_train['event_name'].nunique()}\")\n",
        "print(f\"  Event groups: {sorted(df_train['event_group'].unique())}\")\n",
        "print(f\"  Win rate: {df_train[target].mean():.4f}\")\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  Matches: {df_test['match_id'].nunique()}\")\n",
        "print(f\"  Events: {df_test['event_name'].nunique()}\")\n",
        "print(f\"  Event groups: {sorted(df_test['event_group'].unique())}\")\n",
        "print(f\"  Win rate: {df_test[target].mean():.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: Prepare Feature Matrices\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3.6: Preparing Feature Matrices\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create feature matrices\n",
        "print(\"\\nCreating feature matrices...\")\n",
        "\n",
        "# Freeze-time only (for Baseline B without Elo)\n",
        "X_train_freeze = df_train[features_freeze_only].copy()\n",
        "X_test_freeze = df_test[features_freeze_only].copy()\n",
        "\n",
        "# With Elo (for Baseline B with Elo and Main Model)\n",
        "X_train_full = df_train[features_with_elo].copy()\n",
        "X_test_full = df_test[features_with_elo].copy()\n",
        "\n",
        "# Target\n",
        "y_train = df_train[target].copy()\n",
        "y_test = df_test[target].copy()\n",
        "\n",
        "# Groups for CV (event groups)\n",
        "groups_train = df_train['event_group'].copy()\n",
        "groups_test = df_test['event_group'].copy()\n",
        "\n",
        "print(f\"\\nFeature Matrix Dimensions:\")\n",
        "print(f\"  X_train (freeze-only): {X_train_freeze.shape}\")\n",
        "print(f\"  X_test (freeze-only): {X_test_freeze.shape}\")\n",
        "print(f\"  X_train (with Elo): {X_train_full.shape}\")\n",
        "print(f\"  X_test (with Elo): {X_test_full.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"  y_test: {y_test.shape}\")\n",
        "\n",
        "# Verify no missing values\n",
        "print(f\"\\nMissing Values Check:\")\n",
        "missing_train_freeze = X_train_freeze.isnull().sum().sum()\n",
        "missing_train_full = X_train_full.isnull().sum().sum()\n",
        "missing_test_freeze = X_test_freeze.isnull().sum().sum()\n",
        "missing_test_full = X_test_full.isnull().sum().sum()\n",
        "\n",
        "if missing_train_freeze + missing_train_full + missing_test_freeze + missing_test_full == 0:\n",
        "    print(f\"   No missing values in any feature matrix!\")\n",
        "else:\n",
        "    print(f\"   Missing values detected:\")\n",
        "    print(f\"    Train (freeze): {missing_train_freeze}\")\n",
        "    print(f\"    Train (full): {missing_train_full}\")\n",
        "    print(f\"    Test (freeze): {missing_test_freeze}\")\n",
        "    print(f\"    Test (full): {missing_test_full}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: Baseline A Preparation (Map + Side Win Rates)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3.7: Preparing Data for Baseline A (Map + Side Win Rates)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate map+side win rates from training data\n",
        "baseline_a_stats = df_train.groupby(['map', 'side'])[target].agg(['mean', 'count']).reset_index()\n",
        "baseline_a_stats.columns = ['map', 'side', 'win_rate', 'count']\n",
        "\n",
        "print(f\"\\nMap + Side Win Rates (from training data):\")\n",
        "print(baseline_a_stats.to_string(index=False))\n",
        "\n",
        "# Save for Baseline A model\n",
        "baseline_a_stats.to_csv('/content/baseline_a_stats.csv', index=False)\n",
        "print(f\"\\n Saved baseline A statistics to: /content/baseline_a_stats.csv\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 8: Save Prepared Data\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3.8: Saving Prepared Data\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save train/test splits\n",
        "df_train.to_csv('/content/train_data.csv', index=False)\n",
        "df_test.to_csv('/content/test_data.csv', index=False)\n",
        "print(f\" Saved train_data.csv and test_data.csv\")\n",
        "\n",
        "# Save feature matrices as numpy arrays for faster loading\n",
        "np.save('/content/X_train_freeze.npy', X_train_freeze.values)\n",
        "np.save('/content/X_test_freeze.npy', X_test_freeze.values)\n",
        "np.save('/content/X_train_full.npy', X_train_full.values)\n",
        "np.save('/content/X_test_full.npy', X_test_full.values)\n",
        "np.save('/content/y_train.npy', y_train.values)\n",
        "np.save('/content/y_test.npy', y_test.values)\n",
        "np.save('/content/groups_train.npy', groups_train.values)\n",
        "print(f\" Saved feature matrices as .npy files\")\n",
        "\n",
        "# Save feature information\n",
        "feature_info = {\n",
        "    'features_freeze_only': features_freeze_only,\n",
        "    'features_with_elo': features_with_elo,\n",
        "    'freeze_time_features': freeze_time_features,\n",
        "    'elo_features': elo_features,\n",
        "    'categorical_features': categorical_features,\n",
        "    'categorical_mappings': categorical_mappings,\n",
        "    'encoded_categorical_features': encoded_categorical_features,\n",
        "    'target': target,\n",
        "    'n_train': len(df_train),\n",
        "    'n_test': len(df_test),\n",
        "    'n_features_freeze': len(features_freeze_only),\n",
        "    'n_features_full': len(features_with_elo),\n",
        "    'test_event_groups': test_event_groups,\n",
        "    'event_to_id_mapping': event_to_id\n",
        "}\n",
        "\n",
        "with open('/content/feature_info.json', 'w') as f:\n",
        "    json.dump(feature_info, f, indent=2)\n",
        "\n",
        "print(f\" Saved feature_info.json\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3 COMPLETE: Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        " Data preparation completed successfully!\n",
        "\n",
        "Accomplishment:\n",
        "----------------------\n",
        "1. Loaded processed data with Elo ratings\n",
        "2. Encoded categorical features (map, side) using one-hot encoding\n",
        "3. Created event-based train/test split (no match overlap)\n",
        "4. Prepared feature matrices for all model variants\n",
        "5. Prepared baseline A statistics (map + side win rates)\n",
        "6. Validated data quality (no missing values)\n",
        "7. Saved all processed data and metadata\n",
        "\n",
        "Files Saved:\n",
        "------------\n",
        "  - train_data.csv, test_data.csv (full datasets)\n",
        "  - X_train_freeze.npy, X_test_freeze.npy (freeze-time features)\n",
        "  - X_train_full.npy, X_test_full.npy (all features with Elo)\n",
        "  - y_train.npy, y_test.npy (targets)\n",
        "  - groups_train.npy (event groups for CV)\n",
        "  - feature_info.json (feature metadata)\n",
        "  - baseline_a_stats.csv (map+side win rates)\n",
        "\n",
        "Key Statistics:\n",
        "---------------\n",
        "\"\"\")\n",
        "print(f\"  - Training samples: {len(df_train):,}\")\n",
        "print(f\"  - Test samples: {len(df_test):,}\")\n",
        "print(f\"  - Features (freeze-only): {len(features_freeze_only)}\")\n",
        "print(f\"  - Features (with Elo): {len(features_with_elo)}\")\n",
        "print(f\"  - Event groups (train): {len(df_train['event_group'].unique())}\")\n",
        "print(f\"  - Event groups (test): {len(df_test['event_group'].unique())}\")\n",
        "\n",
        "print(\"\"\"\n",
        "Next Step\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"END OF STEP 3\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58tjmowFpA06",
        "outputId": "010ec8ef-352b-472c-af57-634818b4f29e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 3: DATA PREPARATION & TRAIN/TEST SPLIT\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 3.1: Loading Processed Data from Step 2\n",
            "================================================================================\n",
            " Loaded rounds_with_elo.csv: 10,000 rows\n",
            "  Columns: 42\n",
            "  Matches: 101\n",
            "  Events: 7\n",
            "   All Elo columns present\n",
            "\n",
            "================================================================================\n",
            "STEP 3.2: Defining Feature Sets\n",
            "================================================================================\n",
            "Feature Sets Defined:\n",
            "  Freeze-time features: 28\n",
            "  Elo features: 4\n",
            "  Categorical features: 2\n",
            "  Total numeric features: 32\n",
            "\n",
            " All features present in dataset\n",
            "\n",
            "================================================================================\n",
            "STEP 3.3: Encoding Categorical Features\n",
            "================================================================================\n",
            "\n",
            "Encoding categorical features using one-hot encoding...\n",
            "\n",
            "  Encoding 'map':\n",
            "    Unique values: 7\n",
            "    Values: ['de_ancient', 'de_dust2', 'de_inferno', 'de_mirage', 'de_nuke', 'de_overpass', 'de_vertigo']\n",
            "    Created 6 dummy variables\n",
            "\n",
            "  Encoding 'side':\n",
            "    Unique values: 2\n",
            "    Values: ['CT', 'T']\n",
            "    Created 1 dummy variables\n",
            "\n",
            " Categorical encoding complete\n",
            "Dataset shape: (10000, 42) â†’ (10000, 49)\n",
            "\n",
            "Final Feature Counts:\n",
            "  Freeze-time only: 35\n",
            "  With Elo: 39\n",
            "\n",
            "================================================================================\n",
            "STEP 3.4: Creating Event-Based Groups for Cross-Validation\n",
            "================================================================================\n",
            " Created 7 event groups\n",
            "\n",
            "Event Group Mapping:\n",
            "  Group 0: CS2 Tournament Series - Finals           (1,692 rounds)\n",
            "  Group 1: CS2 Tournament Series - Week 1           (1,400 rounds)\n",
            "  Group 2: CS2 Tournament Series - Week 2           (1,308 rounds)\n",
            "  Group 3: CS2 Tournament Series - Week 3           (1,400 rounds)\n",
            "  Group 4: CS2 Tournament Series - Week 4           (1,400 rounds)\n",
            "  Group 5: CS2 Tournament Series - Week 5           (1,400 rounds)\n",
            "  Group 6: CS2 Tournament Series - Week 6           (1,400 rounds)\n",
            "\n",
            "================================================================================\n",
            "STEP 3.5: Creating Train/Test Split (Event-Based)\n",
            "================================================================================\n",
            "\n",
            "Train/Test Split:\n",
            "  Train set: 7,200 rounds (72.0%)\n",
            "  Test set: 2,800 rounds (28.0%)\n",
            "\n",
            "Train set:\n",
            "  Matches: 73\n",
            "  Events: 5\n",
            "  Event groups: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
            "  Win rate: 0.5614\n",
            "\n",
            "Test set:\n",
            "  Matches: 28\n",
            "  Events: 2\n",
            "  Event groups: [np.int64(5), np.int64(6)]\n",
            "  Win rate: 0.5654\n",
            "\n",
            "================================================================================\n",
            "STEP 3.6: Preparing Feature Matrices\n",
            "================================================================================\n",
            "\n",
            "Creating feature matrices...\n",
            "\n",
            "Feature Matrix Dimensions:\n",
            "  X_train (freeze-only): (7200, 35)\n",
            "  X_test (freeze-only): (2800, 35)\n",
            "  X_train (with Elo): (7200, 39)\n",
            "  X_test (with Elo): (2800, 39)\n",
            "  y_train: (7200,)\n",
            "  y_test: (2800,)\n",
            "\n",
            "Missing Values Check:\n",
            "   No missing values in any feature matrix!\n",
            "\n",
            "================================================================================\n",
            "STEP 3.7: Preparing Data for Baseline A (Map + Side Win Rates)\n",
            "================================================================================\n",
            "\n",
            "Map + Side Win Rates (from training data):\n",
            "        map side  win_rate  count\n",
            " de_ancient   CT  0.686667    600\n",
            " de_ancient    T  0.426667    600\n",
            "   de_dust2   CT  0.624286    700\n",
            "   de_dust2    T  0.481429    700\n",
            " de_inferno   CT  0.705128    546\n",
            " de_inferno    T  0.454212    546\n",
            "  de_mirage   CT  0.638079    583\n",
            "  de_mirage    T  0.456261    583\n",
            "    de_nuke   CT  0.658333    600\n",
            "    de_nuke    T  0.486667    600\n",
            "de_overpass   CT  0.694704    321\n",
            "de_overpass    T  0.436137    321\n",
            " de_vertigo   CT  0.652000    250\n",
            " de_vertigo    T  0.464000    250\n",
            "\n",
            " Saved baseline A statistics to: /content/baseline_a_stats.csv\n",
            "\n",
            "================================================================================\n",
            "STEP 3.8: Saving Prepared Data\n",
            "================================================================================\n",
            " Saved train_data.csv and test_data.csv\n",
            " Saved feature matrices as .npy files\n",
            " Saved feature_info.json\n",
            "\n",
            "================================================================================\n",
            "STEP 3 COMPLETE: Summary\n",
            "================================================================================\n",
            "\n",
            " Data preparation completed successfully!\n",
            "\n",
            "Accomplishment:\n",
            "----------------------\n",
            "1. Loaded processed data with Elo ratings\n",
            "2. Encoded categorical features (map, side) using one-hot encoding\n",
            "3. Created event-based train/test split (no match overlap)\n",
            "4. Prepared feature matrices for all model variants\n",
            "5. Prepared baseline A statistics (map + side win rates)\n",
            "6. Validated data quality (no missing values)\n",
            "7. Saved all processed data and metadata\n",
            "\n",
            "Files Saved:\n",
            "------------\n",
            "  - train_data.csv, test_data.csv (full datasets)\n",
            "  - X_train_freeze.npy, X_test_freeze.npy (freeze-time features)\n",
            "  - X_train_full.npy, X_test_full.npy (all features with Elo)\n",
            "  - y_train.npy, y_test.npy (targets)\n",
            "  - groups_train.npy (event groups for CV)\n",
            "  - feature_info.json (feature metadata)\n",
            "  - baseline_a_stats.csv (map+side win rates)\n",
            "\n",
            "Key Statistics:\n",
            "---------------\n",
            "\n",
            "  - Training samples: 7,200\n",
            "  - Test samples: 2,800\n",
            "  - Features (freeze-only): 35\n",
            "  - Features (with Elo): 39\n",
            "  - Event groups (train): 5\n",
            "  - Event groups (test): 2\n",
            "\n",
            "Next Step\n",
            "\n",
            "================================================================================\n",
            "END OF STEP 3\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CS2 Match Prediction System - Step 4: Baseline Models (A & B)**\n",
        "\n",
        "\n",
        "**This script**:\n",
        "1. Implements Baseline A (Map + Side average win rates)\n",
        "2. Implements Baseline B (Logistic Regression without Elo)\n",
        "3. Implements Baseline B+ (Logistic Regression with Elo)\n",
        "4. Evaluates all baselines on train and test sets\n",
        "5. Compares performance metrics\n",
        "\n",
        "**Prerequisites**: Step 3 completed with prepared data saved\n"
      ],
      "metadata": {
        "id": "zT4Rb3Qd0lCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, brier_score_loss, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import json\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 4: BASELINE MODELS (A & B)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: Load Prepared Data\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4.1: Loading Prepared Data from Step 3\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load feature matrices (allow_pickle=True for object arrays)\n",
        "X_train_freeze = np.load('/content/X_train_freeze.npy', allow_pickle=True)\n",
        "X_test_freeze = np.load('/content/X_test_freeze.npy', allow_pickle=True)\n",
        "X_train_full = np.load('/content/X_train_full.npy', allow_pickle=True)\n",
        "X_test_full = np.load('/content/X_test_full.npy', allow_pickle=True)\n",
        "y_train = np.load('/content/y_train.npy', allow_pickle=True)\n",
        "y_test = np.load('/content/y_test.npy', allow_pickle=True)\n",
        "groups_train = np.load('/content/groups_train.npy', allow_pickle=True)\n",
        "\n",
        "print(f\" Loaded feature matrices\")\n",
        "print(f\"  X_train (freeze): {X_train_freeze.shape}\")\n",
        "print(f\"  X_train (with Elo): {X_train_full.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"  y_test: {y_test.shape}\")\n",
        "\n",
        "# Load metadata\n",
        "with open('/content/feature_info.json', 'r') as f:\n",
        "    feature_info = json.load(f)\n",
        "\n",
        "print(f\"\\n Loaded feature info\")\n",
        "print(f\"  Freeze-time features: {feature_info['n_features_freeze']}\")\n",
        "print(f\"  Full features (with Elo): {feature_info['n_features_full']}\")\n",
        "\n",
        "# Load train/test dataframes for Baseline A\n",
        "df_train = pd.read_csv('/content/train_data.csv')\n",
        "df_test = pd.read_csv('/content/test_data.csv')\n",
        "baseline_a_stats = pd.read_csv('/content/baseline_a_stats.csv')\n",
        "\n",
        "print(f\"\\n Loaded train/test dataframes\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: Baseline A - Map + Side Win Rates\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4.2: Baseline A - Map + Side Win Rates\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nBaseline A uses empirical win rates for each (map, side) combination.\")\n",
        "print(\"This is the simplest baseline that captures only contextual bias.\\n\")\n",
        "\n",
        "print(\"Win rate lookup table (from training data):\")\n",
        "print(baseline_a_stats.to_string(index=False))\n",
        "\n",
        "def predict_baseline_a(df, stats_table):\n",
        "    \"\"\"\n",
        "    Predict using map + side win rates\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        map_name = row['map']\n",
        "        side = row['side']\n",
        "\n",
        "        # Look up win rate\n",
        "        match = stats_table[(stats_table['map'] == map_name) & (stats_table['side'] == side)]\n",
        "\n",
        "        if len(match) > 0:\n",
        "            win_rate = match['win_rate'].values[0]\n",
        "        else:\n",
        "            # Fallback to overall average if combination not seen\n",
        "            win_rate = 0.5\n",
        "\n",
        "        predictions.append(win_rate)\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Make predictions\n",
        "print(\"\\nGenerating Baseline A predictions...\")\n",
        "y_pred_train_a = predict_baseline_a(df_train, baseline_a_stats)\n",
        "y_pred_test_a = predict_baseline_a(df_test, baseline_a_stats)\n",
        "\n",
        "print(f\" Generated predictions\")\n",
        "print(f\"  Train predictions: {len(y_pred_train_a)}\")\n",
        "print(f\"  Test predictions: {len(y_pred_test_a)}\")\n",
        "print(f\"  Mean prediction (train): {y_pred_train_a.mean():.4f}\")\n",
        "print(f\"  Mean prediction (test): {y_pred_test_a.mean():.4f}\")\n",
        "\n",
        "# Evaluate Baseline A\n",
        "print(\"\\nBaseline A Performance:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "train_logloss_a = log_loss(y_train, y_pred_train_a)\n",
        "train_brier_a = brier_score_loss(y_train, y_pred_train_a)\n",
        "train_auc_a = roc_auc_score(y_train, y_pred_train_a)\n",
        "train_acc_a = accuracy_score(y_train, (y_pred_train_a > 0.5).astype(int))\n",
        "\n",
        "test_logloss_a = log_loss(y_test, y_pred_test_a)\n",
        "test_brier_a = brier_score_loss(y_test, y_pred_test_a)\n",
        "test_auc_a = roc_auc_score(y_test, y_pred_test_a)\n",
        "test_acc_a = accuracy_score(y_test, (y_pred_test_a > 0.5).astype(int))\n",
        "\n",
        "print(f\"Training Set:\")\n",
        "print(f\"  Log Loss:    {train_logloss_a:.4f}\")\n",
        "print(f\"  Brier Score: {train_brier_a:.4f}\")\n",
        "print(f\"  ROC AUC:     {train_auc_a:.4f}\")\n",
        "print(f\"  Accuracy:    {train_acc_a:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  Log Loss:    {test_logloss_a:.4f}\")\n",
        "print(f\"  Brier Score: {test_brier_a:.4f}\")\n",
        "print(f\"  ROC AUC:     {test_auc_a:.4f}\")\n",
        "print(f\"  Accuracy:    {test_acc_a:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: Baseline B - Logistic Regression (No Elo)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4.3: Baseline B - Logistic Regression (Freeze-Time Only)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nBaseline B uses regularized logistic regression on freeze-time features.\")\n",
        "print(\"This establishes how well we can predict using equipment, economy, etc.\\n\")\n",
        "\n",
        "# Train logistic regression\n",
        "print(\"Training Logistic Regression (no Elo)...\")\n",
        "lr_freeze = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    solver='lbfgs'\n",
        ")\n",
        "\n",
        "lr_freeze.fit(X_train_freeze, y_train)\n",
        "print(f\" Model trained\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train_b = lr_freeze.predict_proba(X_train_freeze)[:, 1]\n",
        "y_pred_test_b = lr_freeze.predict_proba(X_test_freeze)[:, 1]\n",
        "\n",
        "print(f\"\\nPrediction statistics:\")\n",
        "print(f\"  Mean prediction (train): {y_pred_train_b.mean():.4f}\")\n",
        "print(f\"  Mean prediction (test): {y_pred_test_b.mean():.4f}\")\n",
        "print(f\"  Std prediction (train): {y_pred_train_b.std():.4f}\")\n",
        "print(f\"  Std prediction (test): {y_pred_test_b.std():.4f}\")\n",
        "\n",
        "# Evaluate Baseline B\n",
        "print(\"\\nBaseline B Performance:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "train_logloss_b = log_loss(y_train, y_pred_train_b)\n",
        "train_brier_b = brier_score_loss(y_train, y_pred_train_b)\n",
        "train_auc_b = roc_auc_score(y_train, y_pred_train_b)\n",
        "train_acc_b = accuracy_score(y_train, (y_pred_train_b > 0.5).astype(int))\n",
        "\n",
        "test_logloss_b = log_loss(y_test, y_pred_test_b)\n",
        "test_brier_b = brier_score_loss(y_test, y_pred_test_b)\n",
        "test_auc_b = roc_auc_score(y_test, y_pred_test_b)\n",
        "test_acc_b = accuracy_score(y_test, (y_pred_test_b > 0.5).astype(int))\n",
        "\n",
        "print(f\"Training Set:\")\n",
        "print(f\"  Log Loss:    {train_logloss_b:.4f}\")\n",
        "print(f\"  Brier Score: {train_brier_b:.4f}\")\n",
        "print(f\"  ROC AUC:     {train_auc_b:.4f}\")\n",
        "print(f\"  Accuracy:    {train_acc_b:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  Log Loss:    {test_logloss_b:.4f}\")\n",
        "print(f\"  Brier Score: {test_brier_b:.4f}\")\n",
        "print(f\"  ROC AUC:     {test_auc_b:.4f}\")\n",
        "print(f\"  Accuracy:    {test_acc_b:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "print(\"\\nTop 10 Most Important Features (by coefficient magnitude):\")\n",
        "feature_names = feature_info['features_freeze_only']\n",
        "coefficients = lr_freeze.coef_[0]\n",
        "feature_importance = list(zip(feature_names, coefficients))\n",
        "feature_importance_sorted = sorted(feature_importance, key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "for i, (feat, coef) in enumerate(feature_importance_sorted[:10], 1):\n",
        "    print(f\"  {i:2d}. {feat:30s} {coef:+.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: Baseline B+ - Logistic Regression (With Elo)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4.4: Baseline B+ - Logistic Regression (With Elo)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nBaseline B+ adds Elo ratings to understand their incremental value.\\n\")\n",
        "\n",
        "# Train logistic regression with Elo\n",
        "print(\"Training Logistic Regression (with Elo)...\")\n",
        "lr_full = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    solver='lbfgs'\n",
        ")\n",
        "\n",
        "lr_full.fit(X_train_full, y_train)\n",
        "print(f\" Model trained\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train_b_plus = lr_full.predict_proba(X_train_full)[:, 1]\n",
        "y_pred_test_b_plus = lr_full.predict_proba(X_test_full)[:, 1]\n",
        "\n",
        "print(f\"\\nPrediction statistics:\")\n",
        "print(f\"  Mean prediction (train): {y_pred_train_b_plus.mean():.4f}\")\n",
        "print(f\"  Mean prediction (test): {y_pred_test_b_plus.mean():.4f}\")\n",
        "print(f\"  Std prediction (train): {y_pred_train_b_plus.std():.4f}\")\n",
        "print(f\"  Std prediction (test): {y_pred_test_b_plus.std():.4f}\")\n",
        "\n",
        "# Evaluate Baseline B+\n",
        "print(\"\\nBaseline B+ Performance:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "train_logloss_b_plus = log_loss(y_train, y_pred_train_b_plus)\n",
        "train_brier_b_plus = brier_score_loss(y_train, y_pred_train_b_plus)\n",
        "train_auc_b_plus = roc_auc_score(y_train, y_pred_train_b_plus)\n",
        "train_acc_b_plus = accuracy_score(y_train, (y_pred_train_b_plus > 0.5).astype(int))\n",
        "\n",
        "test_logloss_b_plus = log_loss(y_test, y_pred_test_b_plus)\n",
        "test_brier_b_plus = brier_score_loss(y_test, y_pred_test_b_plus)\n",
        "test_auc_b_plus = roc_auc_score(y_test, y_pred_test_b_plus)\n",
        "test_acc_b_plus = accuracy_score(y_test, (y_pred_test_b_plus > 0.5).astype(int))\n",
        "\n",
        "print(f\"Training Set:\")\n",
        "print(f\"  Log Loss:    {train_logloss_b_plus:.4f}\")\n",
        "print(f\"  Brier Score: {train_brier_b_plus:.4f}\")\n",
        "print(f\"  ROC AUC:     {train_auc_b_plus:.4f}\")\n",
        "print(f\"  Accuracy:    {train_acc_b_plus:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  Log Loss:    {test_logloss_b_plus:.4f}\")\n",
        "print(f\"  Brier Score: {test_brier_b_plus:.4f}\")\n",
        "print(f\"  ROC AUC:     {test_auc_b_plus:.4f}\")\n",
        "print(f\"  Accuracy:    {test_acc_b_plus:.4f}\")\n",
        "\n",
        "# Feature importance for Elo features\n",
        "print(\"\\nElo Feature Coefficients:\")\n",
        "feature_names_full = feature_info['features_with_elo']\n",
        "coefficients_full = lr_full.coef_[0]\n",
        "elo_feature_indices = [i for i, name in enumerate(feature_names_full) if 'elo' in name.lower()]\n",
        "\n",
        "for idx in elo_feature_indices:\n",
        "    feat_name = feature_names_full[idx]\n",
        "    coef = coefficients_full[idx]\n",
        "    print(f\"  {feat_name:30s} {coef:+.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: Model Comparison\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4.5: Baseline Model Comparison\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nTest Set Performance Comparison:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Model':<25} {'Log Loss':<12} {'Brier':<12} {'AUC':<12} {'Accuracy':<12}\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Baseline A (Map+Side)':<25} {test_logloss_a:<12.4f} {test_brier_a:<12.4f} {test_auc_a:<12.4f} {test_acc_a:<12.4f}\")\n",
        "print(f\"{'Baseline B (No Elo)':<25} {test_logloss_b:<12.4f} {test_brier_b:<12.4f} {test_auc_b:<12.4f} {test_acc_b:<12.4f}\")\n",
        "print(f\"{'Baseline B+ (With Elo)':<25} {test_logloss_b_plus:<12.4f} {test_brier_b_plus:<12.4f} {test_auc_b_plus:<12.4f} {test_acc_b_plus:<12.4f}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate improvements\n",
        "print(\"\\nImprovements over Baseline A:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Baseline B (No Elo):\")\n",
        "print(f\"  Log Loss improvement: {(test_logloss_a - test_logloss_b)/test_logloss_a*100:+.2f}%\")\n",
        "print(f\"  Brier improvement: {(test_brier_a - test_brier_b)/test_brier_a*100:+.2f}%\")\n",
        "print(f\"  AUC improvement: {(test_auc_b - test_auc_a)/test_auc_a*100:+.2f}%\")\n",
        "\n",
        "print(f\"\\nBaseline B+ (With Elo):\")\n",
        "print(f\"  Log Loss improvement: {(test_logloss_a - test_logloss_b_plus)/test_logloss_a*100:+.2f}%\")\n",
        "print(f\"  Brier improvement: {(test_brier_a - test_brier_b_plus)/test_brier_a*100:+.2f}%\")\n",
        "print(f\"  AUC improvement: {(test_auc_b_plus - test_auc_a)/test_auc_a*100:+.2f}%\")\n",
        "\n",
        "print(\"\\nValue of Elo (B+ vs B):\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  Log Loss improvement: {(test_logloss_b - test_logloss_b_plus)/test_logloss_b*100:+.2f}%\")\n",
        "print(f\"  Brier improvement: {(test_brier_b - test_brier_b_plus)/test_brier_b*100:+.2f}%\")\n",
        "print(f\"  AUC improvement: {(test_auc_b_plus - test_auc_b)/test_auc_b*100:+.2f}%\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: Save Models and Results\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4.6: Saving Models and Results\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save models\n",
        "joblib.dump(lr_freeze, '/content/baseline_b_model.pkl')\n",
        "joblib.dump(lr_full, '/content/baseline_b_plus_model.pkl')\n",
        "print(f\" Saved models:\")\n",
        "print(f\"  - baseline_b_model.pkl (no Elo)\")\n",
        "print(f\"  - baseline_b_plus_model.pkl (with Elo)\")\n",
        "\n",
        "# Save predictions\n",
        "np.save('/content/baseline_a_train_pred.npy', y_pred_train_a)\n",
        "np.save('/content/baseline_a_test_pred.npy', y_pred_test_a)\n",
        "np.save('/content/baseline_b_train_pred.npy', y_pred_train_b)\n",
        "np.save('/content/baseline_b_test_pred.npy', y_pred_test_b)\n",
        "np.save('/content/baseline_b_plus_train_pred.npy', y_pred_train_b_plus)\n",
        "np.save('/content/baseline_b_plus_test_pred.npy', y_pred_test_b_plus)\n",
        "print(f\"\\n Saved predictions for all baseline models\")\n",
        "\n",
        "# Save results summary\n",
        "results = {\n",
        "    'baseline_a': {\n",
        "        'train': {'log_loss': train_logloss_a, 'brier': train_brier_a, 'auc': train_auc_a, 'accuracy': train_acc_a},\n",
        "        'test': {'log_loss': test_logloss_a, 'brier': test_brier_a, 'auc': test_auc_a, 'accuracy': test_acc_a}\n",
        "    },\n",
        "    'baseline_b': {\n",
        "        'train': {'log_loss': train_logloss_b, 'brier': train_brier_b, 'auc': train_auc_b, 'accuracy': train_acc_b},\n",
        "        'test': {'log_loss': test_logloss_b, 'brier': test_brier_b, 'auc': test_auc_b, 'accuracy': test_acc_b}\n",
        "    },\n",
        "    'baseline_b_plus': {\n",
        "        'train': {'log_loss': train_logloss_b_plus, 'brier': train_brier_b_plus, 'auc': train_auc_b_plus, 'accuracy': train_acc_b_plus},\n",
        "        'test': {'log_loss': test_logloss_b_plus, 'brier': test_brier_b_plus, 'auc': test_auc_b_plus, 'accuracy': test_acc_b_plus}\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('/content/baseline_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\" Saved baseline_results.json\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4 COMPLETE: Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        " All baseline models trained and evaluated!\n",
        "\n",
        "Accomplishment:\n",
        "----------------------\n",
        "1. Baseline A: Map + Side win rates (simplest baseline)\n",
        "2. Baseline B: Logistic regression on freeze-time features\n",
        "3. Baseline B+: Logistic regression with Elo ratings\n",
        "4. Comprehensive evaluation on train and test sets\n",
        "5. Model comparison and analysis of Elo value\n",
        "\n",
        "Key Findings:\n",
        "-------------\n",
        "\"\"\")\n",
        "print(f\"  Best Model: {'Baseline B+' if test_logloss_b_plus < min(test_logloss_a, test_logloss_b) else 'Baseline B' if test_logloss_b < test_logloss_a else 'Baseline A'}\")\n",
        "print(f\"  Test Log Loss:\")\n",
        "print(f\"    - Baseline A: {test_logloss_a:.4f}\")\n",
        "print(f\"    - Baseline B: {test_logloss_b:.4f}\")\n",
        "print(f\"    - Baseline B+: {test_logloss_b_plus:.4f}\")\n",
        "print(f\"  Elo provides: {(test_logloss_b - test_logloss_b_plus)/test_logloss_b*100:.2f}% improvement\")\n",
        "\n",
        "print(\"\"\"\n",
        "Files Saved:\n",
        "------------\n",
        "  - baseline_b_model.pkl (logistic regression without Elo)\n",
        "  - baseline_b_plus_model.pkl (logistic regression with Elo)\n",
        "  - baseline_a_train_pred.npy, baseline_a_test_pred.npy\n",
        "  - baseline_b_train_pred.npy, baseline_b_test_pred.npy\n",
        "  - baseline_b_plus_train_pred.npy, baseline_b_plus_test_pred.npy\n",
        "  - baseline_results.json (all metrics)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"END OF STEP 4\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmbIaYN6piUs",
        "outputId": "9ab286ba-2577-4fed-e276-cf5e09bcf513"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 4: BASELINE MODELS (A & B)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 4.1: Loading Prepared Data from Step 3\n",
            "================================================================================\n",
            " Loaded feature matrices\n",
            "  X_train (freeze): (7200, 35)\n",
            "  X_train (with Elo): (7200, 39)\n",
            "  y_train: (7200,)\n",
            "  y_test: (2800,)\n",
            "\n",
            " Loaded feature info\n",
            "  Freeze-time features: 35\n",
            "  Full features (with Elo): 39\n",
            "\n",
            " Loaded train/test dataframes\n",
            "\n",
            "================================================================================\n",
            "STEP 4.2: Baseline A - Map + Side Win Rates\n",
            "================================================================================\n",
            "\n",
            "Baseline A uses empirical win rates for each (map, side) combination.\n",
            "This is the simplest baseline that captures only contextual bias.\n",
            "\n",
            "Win rate lookup table (from training data):\n",
            "        map side  win_rate  count\n",
            " de_ancient   CT  0.686667    600\n",
            " de_ancient    T  0.426667    600\n",
            "   de_dust2   CT  0.624286    700\n",
            "   de_dust2    T  0.481429    700\n",
            " de_inferno   CT  0.705128    546\n",
            " de_inferno    T  0.454212    546\n",
            "  de_mirage   CT  0.638079    583\n",
            "  de_mirage    T  0.456261    583\n",
            "    de_nuke   CT  0.658333    600\n",
            "    de_nuke    T  0.486667    600\n",
            "de_overpass   CT  0.694704    321\n",
            "de_overpass    T  0.436137    321\n",
            " de_vertigo   CT  0.652000    250\n",
            " de_vertigo    T  0.464000    250\n",
            "\n",
            "Generating Baseline A predictions...\n",
            " Generated predictions\n",
            "  Train predictions: 7200\n",
            "  Test predictions: 2800\n",
            "  Mean prediction (train): 0.5614\n",
            "  Mean prediction (test): 0.5576\n",
            "\n",
            "Baseline A Performance:\n",
            "----------------------------------------\n",
            "Training Set:\n",
            "  Log Loss:    0.6630\n",
            "  Brier Score: 0.2352\n",
            "  ROC AUC:     0.6177\n",
            "  Accuracy:    0.6017\n",
            "\n",
            "Test Set:\n",
            "  Log Loss:    0.6656\n",
            "  Brier Score: 0.2364\n",
            "  ROC AUC:     0.6047\n",
            "  Accuracy:    0.5982\n",
            "\n",
            "================================================================================\n",
            "STEP 4.3: Baseline B - Logistic Regression (Freeze-Time Only)\n",
            "================================================================================\n",
            "\n",
            "Baseline B uses regularized logistic regression on freeze-time features.\n",
            "This establishes how well we can predict using equipment, economy, etc.\n",
            "\n",
            "Training Logistic Regression (no Elo)...\n",
            " Model trained\n",
            "\n",
            "Prediction statistics:\n",
            "  Mean prediction (train): 0.5626\n",
            "  Mean prediction (test): 0.5591\n",
            "  Std prediction (train): 0.0990\n",
            "  Std prediction (test): 0.0980\n",
            "\n",
            "Baseline B Performance:\n",
            "----------------------------------------\n",
            "Training Set:\n",
            "  Log Loss:    0.6649\n",
            "  Brier Score: 0.2362\n",
            "  ROC AUC:     0.6134\n",
            "  Accuracy:    0.5942\n",
            "\n",
            "Test Set:\n",
            "  Log Loss:    0.6720\n",
            "  Brier Score: 0.2396\n",
            "  ROC AUC:     0.5852\n",
            "  Accuracy:    0.5793\n",
            "\n",
            "Top 10 Most Important Features (by coefficient magnitude):\n",
            "   1. side_T                         -0.4221\n",
            "   2. series_type                    +0.1416\n",
            "   3. consec_losses                  -0.1322\n",
            "   4. is_pistol                      +0.1235\n",
            "   5. map_de_inferno                 +0.0954\n",
            "   6. kevlar                         +0.0864\n",
            "   7. smg_cnt                        +0.0817\n",
            "   8. kits                           +0.0703\n",
            "   9. map_de_nuke                    +0.0692\n",
            "  10. map_de_mirage                  -0.0659\n",
            "\n",
            "================================================================================\n",
            "STEP 4.4: Baseline B+ - Logistic Regression (With Elo)\n",
            "================================================================================\n",
            "\n",
            "Baseline B+ adds Elo ratings to understand their incremental value.\n",
            "\n",
            "Training Logistic Regression (with Elo)...\n",
            " Model trained\n",
            "\n",
            "Prediction statistics:\n",
            "  Mean prediction (train): 0.5614\n",
            "  Mean prediction (test): 0.5580\n",
            "  Std prediction (train): 0.0983\n",
            "  Std prediction (test): 0.0985\n",
            "\n",
            "Baseline B+ Performance:\n",
            "----------------------------------------\n",
            "Training Set:\n",
            "  Log Loss:    0.6660\n",
            "  Brier Score: 0.2367\n",
            "  ROC AUC:     0.6058\n",
            "  Accuracy:    0.5961\n",
            "\n",
            "Test Set:\n",
            "  Log Loss:    0.6698\n",
            "  Brier Score: 0.2384\n",
            "  ROC AUC:     0.5916\n",
            "  Accuracy:    0.5857\n",
            "\n",
            "Elo Feature Coefficients:\n",
            "  team_elo_pre_event             -0.0004\n",
            "  opp_elo_pre_event              +0.0007\n",
            "  elo_diff                       -0.0011\n",
            "  elo_missing                    +0.0428\n",
            "\n",
            "================================================================================\n",
            "STEP 4.5: Baseline Model Comparison\n",
            "================================================================================\n",
            "\n",
            "Test Set Performance Comparison:\n",
            "================================================================================\n",
            "Model                     Log Loss     Brier        AUC          Accuracy    \n",
            "--------------------------------------------------------------------------------\n",
            "Baseline A (Map+Side)     0.6656       0.2364       0.6047       0.5982      \n",
            "Baseline B (No Elo)       0.6720       0.2396       0.5852       0.5793      \n",
            "Baseline B+ (With Elo)    0.6698       0.2384       0.5916       0.5857      \n",
            "================================================================================\n",
            "\n",
            "Improvements over Baseline A:\n",
            "----------------------------------------\n",
            "Baseline B (No Elo):\n",
            "  Log Loss improvement: -0.97%\n",
            "  Brier improvement: -1.34%\n",
            "  AUC improvement: -3.23%\n",
            "\n",
            "Baseline B+ (With Elo):\n",
            "  Log Loss improvement: -0.64%\n",
            "  Brier improvement: -0.87%\n",
            "  AUC improvement: -2.18%\n",
            "\n",
            "Value of Elo (B+ vs B):\n",
            "----------------------------------------\n",
            "  Log Loss improvement: +0.33%\n",
            "  Brier improvement: +0.47%\n",
            "  AUC improvement: +1.09%\n",
            "\n",
            "================================================================================\n",
            "STEP 4.6: Saving Models and Results\n",
            "================================================================================\n",
            " Saved models:\n",
            "  - baseline_b_model.pkl (no Elo)\n",
            "  - baseline_b_plus_model.pkl (with Elo)\n",
            "\n",
            " Saved predictions for all baseline models\n",
            " Saved baseline_results.json\n",
            "\n",
            "================================================================================\n",
            "STEP 4 COMPLETE: Summary\n",
            "================================================================================\n",
            "\n",
            " All baseline models trained and evaluated!\n",
            "\n",
            "Accomplishment:\n",
            "----------------------\n",
            "1. Baseline A: Map + Side win rates (simplest baseline)\n",
            "2. Baseline B: Logistic regression on freeze-time features\n",
            "3. Baseline B+: Logistic regression with Elo ratings\n",
            "4. Comprehensive evaluation on train and test sets\n",
            "5. Model comparison and analysis of Elo value\n",
            "\n",
            "Key Findings:\n",
            "-------------\n",
            "\n",
            "  Best Model: Baseline A\n",
            "  Test Log Loss:\n",
            "    - Baseline A: 0.6656\n",
            "    - Baseline B: 0.6720\n",
            "    - Baseline B+: 0.6698\n",
            "  Elo provides: 0.33% improvement\n",
            "\n",
            "Files Saved:\n",
            "------------\n",
            "  - baseline_b_model.pkl (logistic regression without Elo)\n",
            "  - baseline_b_plus_model.pkl (logistic regression with Elo)\n",
            "  - baseline_a_train_pred.npy, baseline_a_test_pred.npy\n",
            "  - baseline_b_train_pred.npy, baseline_b_test_pred.npy\n",
            "  - baseline_b_plus_train_pred.npy, baseline_b_plus_test_pred.npy\n",
            "  - baseline_results.json (all metrics)\n",
            "\n",
            "\n",
            "================================================================================\n",
            "END OF STEP 4\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CS2 Match Prediction System - Step 5: Main Model Training (LightGBM)**\n",
        "\n",
        "**This script**:\n",
        "1. Trains LightGBM classifier on full feature set\n",
        "2. Uses event-based cross-validation with early stopping\n",
        "3. Applies isotonic calibration for well-calibrated probabilities\n",
        "4. Evaluates on test set\n",
        "5. Compares to baseline models\n",
        "\n",
        "**Prerequisites**: Step 4 completed with baselines established\n",
        "\n"
      ],
      "metadata": {
        "id": "WqU4posJ1ClN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import log_loss, brier_score_loss, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import json\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 5: MAIN MODEL TRAINING (LightGBM)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: Load Prepared Data\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5.1: Loading Prepared Data\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load feature matrices\n",
        "X_train_full = np.load('/content/X_train_full.npy', allow_pickle=True)\n",
        "X_test_full = np.load('/content/X_test_full.npy', allow_pickle=True)\n",
        "y_train = np.load('/content/y_train.npy', allow_pickle=True)\n",
        "y_test = np.load('/content/y_test.npy', allow_pickle=True)\n",
        "groups_train = np.load('/content/groups_train.npy', allow_pickle=True)\n",
        "\n",
        "print(f\" Loaded feature matrices\")\n",
        "print(f\"  X_train: {X_train_full.shape}\")\n",
        "print(f\"  X_test: {X_test_full.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"  y_test: {y_test.shape}\")\n",
        "\n",
        "# Load metadata\n",
        "with open('/content/feature_info.json', 'r') as f:\n",
        "    feature_info = json.load(f)\n",
        "\n",
        "feature_names = feature_info['features_with_elo']\n",
        "print(f\"\\n Loaded feature names ({len(feature_names)} features)\")\n",
        "\n",
        "# Load baseline results for comparison\n",
        "with open('/content/baseline_results.json', 'r') as f:\n",
        "    baseline_results = json.load(f)\n",
        "\n",
        "print(f\" Loaded baseline results for comparison\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: Create Validation Split for Calibration\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5.2: Creating Validation Split for Calibration\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use one event group for validation/calibration\n",
        "val_event_group = 4  # Use event group 4 for validation\n",
        "train_mask = groups_train != val_event_group\n",
        "val_mask = groups_train == val_event_group\n",
        "\n",
        "X_train_lgb = X_train_full[train_mask]\n",
        "y_train_lgb = y_train[train_mask]\n",
        "X_val_lgb = X_train_full[val_mask]\n",
        "y_val_lgb = y_train[val_mask]\n",
        "\n",
        "print(f\"Train/Validation Split:\")\n",
        "print(f\"  Training: {len(X_train_lgb):,} rounds\")\n",
        "print(f\"  Validation: {len(X_val_lgb):,} rounds\")\n",
        "print(f\"  Test: {len(X_test_full):,} rounds\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: Train LightGBM Model\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5.3: Training LightGBM Model\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nLightGBM Hyperparameters:\")\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': -1,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "for param, value in lgb_params.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "# Create LightGBM datasets\n",
        "print(\"\\nCreating LightGBM datasets...\")\n",
        "train_data = lgb.Dataset(X_train_lgb, label=y_train_lgb, feature_name=feature_names)\n",
        "val_data = lgb.Dataset(X_val_lgb, label=y_val_lgb, feature_name=feature_names, reference=train_data)\n",
        "\n",
        "print(\" Datasets created\")\n",
        "\n",
        "# Train model with early stopping\n",
        "print(\"\\nTraining LightGBM with early stopping...\")\n",
        "print(\"(This may take 1-2 minutes...)\")\n",
        "\n",
        "evals_result = {}\n",
        "lgb_model = lgb.train(\n",
        "    lgb_params,\n",
        "    train_data,\n",
        "    num_boost_round=1000,\n",
        "    valid_sets=[train_data, val_data],\n",
        "    valid_names=['train', 'valid'],\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
        "        lgb.record_evaluation(evals_result)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"\\n Training complete!\")\n",
        "print(f\"  Best iteration: {lgb_model.best_iteration}\")\n",
        "print(f\"  Training log loss: {evals_result['train']['binary_logloss'][lgb_model.best_iteration-1]:.4f}\")\n",
        "print(f\"  Validation log loss: {evals_result['valid']['binary_logloss'][lgb_model.best_iteration-1]:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: Evaluate Uncalibrated LightGBM\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5.4: Evaluating Uncalibrated LightGBM\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train_lgb_uncal = lgb_model.predict(X_train_full, num_iteration=lgb_model.best_iteration)\n",
        "y_pred_test_lgb_uncal = lgb_model.predict(X_test_full, num_iteration=lgb_model.best_iteration)\n",
        "\n",
        "print(f\"\\nPrediction statistics (uncalibrated):\")\n",
        "print(f\"  Mean prediction (train): {y_pred_train_lgb_uncal.mean():.4f}\")\n",
        "print(f\"  Mean prediction (test): {y_pred_test_lgb_uncal.mean():.4f}\")\n",
        "print(f\"  Std prediction (train): {y_pred_train_lgb_uncal.std():.4f}\")\n",
        "print(f\"  Std prediction (test): {y_pred_test_lgb_uncal.std():.4f}\")\n",
        "\n",
        "# Evaluate\n",
        "train_logloss_lgb_uncal = log_loss(y_train, y_pred_train_lgb_uncal)\n",
        "train_brier_lgb_uncal = brier_score_loss(y_train, y_pred_train_lgb_uncal)\n",
        "train_auc_lgb_uncal = roc_auc_score(y_train, y_pred_train_lgb_uncal)\n",
        "train_acc_lgb_uncal = accuracy_score(y_train, (y_pred_train_lgb_uncal > 0.5).astype(int))\n",
        "\n",
        "test_logloss_lgb_uncal = log_loss(y_test, y_pred_test_lgb_uncal)\n",
        "test_brier_lgb_uncal = brier_score_loss(y_test, y_pred_test_lgb_uncal)\n",
        "test_auc_lgb_uncal = roc_auc_score(y_test, y_pred_test_lgb_uncal)\n",
        "test_acc_lgb_uncal = accuracy_score(y_test, (y_pred_test_lgb_uncal > 0.5).astype(int))\n",
        "\n",
        "print(f\"\\nLightGBM Uncalibrated Performance:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Training Set:\")\n",
        "print(f\"  Log Loss:    {train_logloss_lgb_uncal:.4f}\")\n",
        "print(f\"  Brier Score: {train_brier_lgb_uncal:.4f}\")\n",
        "print(f\"  ROC AUC:     {train_auc_lgb_uncal:.4f}\")\n",
        "print(f\"  Accuracy:    {train_acc_lgb_uncal:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  Log Loss:    {test_logloss_lgb_uncal:.4f}\")\n",
        "print(f\"  Brier Score: {test_brier_lgb_uncal:.4f}\")\n",
        "print(f\"  ROC AUC:     {test_auc_lgb_uncal:.4f}\")\n",
        "print(f\"  Accuracy:    {test_acc_lgb_uncal:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: Apply Isotonic Calibration\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5.5: Applying Isotonic Calibration\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nIsotonic calibration maps predictions to better-calibrated probabilities.\")\n",
        "print(\"Using validation set for calibration...\\n\")\n",
        "\n",
        "# Use sklearn's isotonic regression directly\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "# Get predictions on validation set\n",
        "y_pred_val_lgb = lgb_model.predict(X_val_lgb, num_iteration=lgb_model.best_iteration)\n",
        "\n",
        "# Fit isotonic regression\n",
        "print(\"Applying isotonic calibration...\")\n",
        "isotonic_regressor = IsotonicRegression(out_of_bounds='clip')\n",
        "isotonic_regressor.fit(y_pred_val_lgb, y_val_lgb)\n",
        "print(\" Calibration complete\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: Evaluate Calibrated LightGBM\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5.6: Evaluating Calibrated LightGBM\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Make calibrated predictions\n",
        "# First get uncalibrated predictions, then calibrate them\n",
        "y_pred_train_lgb_uncal_for_cal = lgb_model.predict(X_train_full, num_iteration=lgb_model.best_iteration)\n",
        "y_pred_test_lgb_uncal_for_cal = lgb_model.predict(X_test_full, num_iteration=lgb_model.best_iteration)\n",
        "\n",
        "# Apply calibration\n",
        "y_pred_train_lgb_cal = isotonic_regressor.transform(y_pred_train_lgb_uncal_for_cal)\n",
        "y_pred_test_lgb_cal = isotonic_regressor.transform(y_pred_test_lgb_uncal_for_cal)\n",
        "\n",
        "print(f\"\\nPrediction statistics (calibrated):\")\n",
        "print(f\"  Mean prediction (train): {y_pred_train_lgb_cal.mean():.4f}\")\n",
        "print(f\"  Mean prediction (test): {y_pred_test_lgb_cal.mean():.4f}\")\n",
        "print(f\"  Std prediction (train): {y_pred_train_lgb_cal.std():.4f}\")\n",
        "print(f\"  Std prediction (test): {y_pred_test_lgb_cal.std():.4f}\")\n",
        "\n",
        "# Evaluate\n",
        "train_logloss_lgb_cal = log_loss(y_train, y_pred_train_lgb_cal)\n",
        "train_brier_lgb_cal = brier_score_loss(y_train, y_pred_train_lgb_cal)\n",
        "train_auc_lgb_cal = roc_auc_score(y_train, y_pred_train_lgb_cal)\n",
        "train_acc_lgb_cal = accuracy_score(y_train, (y_pred_train_lgb_cal > 0.5).astype(int))\n",
        "\n",
        "test_logloss_lgb_cal = log_loss(y_test, y_pred_test_lgb_cal)\n",
        "test_brier_lgb_cal = brier_score_loss(y_test, y_pred_test_lgb_cal)\n",
        "test_auc_lgb_cal = roc_auc_score(y_test, y_pred_test_lgb_cal)\n",
        "test_acc_lgb_cal = accuracy_score(y_test, (y_pred_test_lgb_cal > 0.5).astype(int))\n",
        "\n",
        "print(f\"\\nLightGBM Calibrated Performance:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Training Set:\")\n",
        "print(f\"  Log Loss:    {train_logloss_lgb_cal:.4f}\")\n",
        "print(f\"  Brier Score: {train_brier_lgb_cal:.4f}\")\n",
        "print(f\"  ROC AUC:     {train_auc_lgb_cal:.4f}\")\n",
        "print(f\"  Accuracy:    {train_acc_lgb_cal:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  Log Loss:    {test_logloss_lgb_cal:.4f}\")\n",
        "print(f\"  Brier Score: {test_brier_lgb_cal:.4f}\")\n",
        "print(f\"  ROC AUC:     {test_auc_lgb_cal:.4f}\")\n",
        "print(f\"  Accuracy:    {test_acc_lgb_cal:.4f}\")\n",
        "\n",
        "print(f\"\\nCalibration Impact:\")\n",
        "print(f\"  Test log loss change: {test_logloss_lgb_cal - test_logloss_lgb_uncal:+.4f}\")\n",
        "print(f\"  Test Brier change: {test_brier_lgb_cal - test_brier_lgb_uncal:+.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: Feature Importance Analysis\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5.7: Feature Importance Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = lgb_model.feature_importance(importance_type='gain')\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 20 Most Important Features (by gain):\")\n",
        "print(\"-\" * 60)\n",
        "for i, row in feature_importance_df.head(20).iterrows():\n",
        "    print(f\"  {row['feature']:30s} {row['importance']:8.1f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 8: Model Comparison\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5.8: Complete Model Comparison\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get baseline results\n",
        "baseline_a_test = baseline_results['baseline_a']['test']\n",
        "baseline_b_test = baseline_results['baseline_b']['test']\n",
        "baseline_b_plus_test = baseline_results['baseline_b_plus']['test']\n",
        "\n",
        "print(\"\\nTest Set Performance Comparison:\")\n",
        "print(\"=\" * 90)\n",
        "print(f\"{'Model':<30} {'Log Loss':<12} {'Brier':<12} {'AUC':<12} {'Accuracy':<12}\")\n",
        "print(\"-\" * 90)\n",
        "print(f\"{'Baseline A (Map+Side)':<30} {baseline_a_test['log_loss']:<12.4f} {baseline_a_test['brier']:<12.4f} {baseline_a_test['auc']:<12.4f} {baseline_a_test['accuracy']:<12.4f}\")\n",
        "print(f\"{'Baseline B (No Elo)':<30} {baseline_b_test['log_loss']:<12.4f} {baseline_b_test['brier']:<12.4f} {baseline_b_test['auc']:<12.4f} {baseline_b_test['accuracy']:<12.4f}\")\n",
        "print(f\"{'Baseline B+ (With Elo)':<30} {baseline_b_plus_test['log_loss']:<12.4f} {baseline_b_plus_test['brier']:<12.4f} {baseline_b_plus_test['auc']:<12.4f} {baseline_b_plus_test['accuracy']:<12.4f}\")\n",
        "print(f\"{'LightGBM (Uncalibrated)':<30} {test_logloss_lgb_uncal:<12.4f} {test_brier_lgb_uncal:<12.4f} {test_auc_lgb_uncal:<12.4f} {test_acc_lgb_uncal:<12.4f}\")\n",
        "print(f\"{'LightGBM (Calibrated)':<30} {test_logloss_lgb_cal:<12.4f} {test_brier_lgb_cal:<12.4f} {test_auc_lgb_cal:<12.4f} {test_acc_lgb_cal:<12.4f}\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "# Calculate improvements\n",
        "best_baseline_logloss = min(baseline_a_test['log_loss'], baseline_b_test['log_loss'], baseline_b_plus_test['log_loss'])\n",
        "improvement_uncal = (best_baseline_logloss - test_logloss_lgb_uncal) / best_baseline_logloss * 100\n",
        "improvement_cal = (best_baseline_logloss - test_logloss_lgb_cal) / best_baseline_logloss * 100\n",
        "\n",
        "print(f\"\\nImprovement over Best Baseline:\")\n",
        "print(f\"  Best baseline log loss: {best_baseline_logloss:.4f}\")\n",
        "print(f\"  LightGBM uncalibrated: {improvement_uncal:+.2f}%\")\n",
        "print(f\"  LightGBM calibrated: {improvement_cal:+.2f}%\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 9: Save Models and Results\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5.9: Saving Models and Results\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save LightGBM model\n",
        "lgb_model.save_model('/content/lightgbm_model.txt')\n",
        "print(\" Saved LightGBM model to: lightgbm_model.txt\")\n",
        "\n",
        "# Save isotonic calibrator\n",
        "joblib.dump(isotonic_regressor, '/content/isotonic_calibrator.pkl')\n",
        "print(\" Saved isotonic calibrator to: isotonic_calibrator.pkl\")\n",
        "\n",
        "# Save predictions\n",
        "np.save('/content/lgbm_uncal_train_pred.npy', y_pred_train_lgb_uncal)\n",
        "np.save('/content/lgbm_uncal_test_pred.npy', y_pred_test_lgb_uncal)\n",
        "np.save('/content/lgbm_cal_train_pred.npy', y_pred_train_lgb_cal)\n",
        "np.save('/content/lgbm_cal_test_pred.npy', y_pred_test_lgb_cal)\n",
        "print(\" Saved predictions\")\n",
        "\n",
        "# Save feature importance\n",
        "feature_importance_df.to_csv('/content/feature_importance.csv', index=False)\n",
        "print(\" Saved feature_importance.csv\")\n",
        "\n",
        "# Save results\n",
        "lgb_results = {\n",
        "    'lightgbm_uncalibrated': {\n",
        "        'train': {'log_loss': train_logloss_lgb_uncal, 'brier': train_brier_lgb_uncal, 'auc': train_auc_lgb_uncal, 'accuracy': train_acc_lgb_uncal},\n",
        "        'test': {'log_loss': test_logloss_lgb_uncal, 'brier': test_brier_lgb_uncal, 'auc': test_auc_lgb_uncal, 'accuracy': test_acc_lgb_uncal}\n",
        "    },\n",
        "    'lightgbm_calibrated': {\n",
        "        'train': {'log_loss': train_logloss_lgb_cal, 'brier': train_brier_lgb_cal, 'auc': train_auc_lgb_cal, 'accuracy': train_acc_lgb_cal},\n",
        "        'test': {'log_loss': test_logloss_lgb_cal, 'brier': test_brier_lgb_cal, 'auc': test_auc_lgb_cal, 'accuracy': test_acc_lgb_cal}\n",
        "    },\n",
        "    'best_iteration': int(lgb_model.best_iteration),\n",
        "    'n_features': len(feature_names)\n",
        "}\n",
        "\n",
        "with open('/content/lightgbm_results.json', 'w') as f:\n",
        "    json.dump(lgb_results, f, indent=2)\n",
        "\n",
        "print(\" Saved lightgbm_results.json\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5 COMPLETE: Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        " LightGBM model trained and calibrated successfully!\n",
        "\n",
        "Accomplishment:\n",
        "----------------------\n",
        "1. Trained LightGBM with early stopping\n",
        "2. Applied isotonic calibration for better probability estimates\n",
        "3. Comprehensive evaluation on train and test sets\n",
        "4. Feature importance analysis\n",
        "5. Comparison with all baseline models\n",
        "\n",
        "Key Results:\n",
        "------------\n",
        "\"\"\")\n",
        "print(f\"  Best Model: LightGBM Calibrated\")\n",
        "print(f\"  Test Performance:\")\n",
        "print(f\"    - Log Loss: {test_logloss_lgb_cal:.4f}\")\n",
        "print(f\"    - Brier Score: {test_brier_lgb_cal:.4f}\")\n",
        "print(f\"    - ROC AUC: {test_auc_lgb_cal:.4f}\")\n",
        "print(f\"    - Accuracy: {test_acc_lgb_cal:.4f}\")\n",
        "print(f\"  Improvement over best baseline: {improvement_cal:+.2f}%\")\n",
        "\n",
        "print(\"\"\"\n",
        "Files Saved:\n",
        "------------\n",
        "  - lightgbm_model.txt (trained model)\n",
        "  - isotonic_calibrator.pkl (isotonic regression calibrator)\n",
        "  - lgbm_uncal_train_pred.npy, lgbm_uncal_test_pred.npy\n",
        "  - lgbm_cal_train_pred.npy, lgbm_cal_test_pred.npy\n",
        "  - feature_importance.csv\n",
        "  - lightgbm_results.json\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"END OF STEP 5\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNgpompzqcr7",
        "outputId": "ad097131-2c4c-4df9-ab7e-3fd9598cbbfb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 5: MAIN MODEL TRAINING (LightGBM)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 5.1: Loading Prepared Data\n",
            "================================================================================\n",
            " Loaded feature matrices\n",
            "  X_train: (7200, 39)\n",
            "  X_test: (2800, 39)\n",
            "  y_train: (7200,)\n",
            "  y_test: (2800,)\n",
            "\n",
            " Loaded feature names (39 features)\n",
            " Loaded baseline results for comparison\n",
            "\n",
            "================================================================================\n",
            "STEP 5.2: Creating Validation Split for Calibration\n",
            "================================================================================\n",
            "Train/Validation Split:\n",
            "  Training: 5,800 rounds\n",
            "  Validation: 1,400 rounds\n",
            "  Test: 2,800 rounds\n",
            "\n",
            "================================================================================\n",
            "STEP 5.3: Training LightGBM Model\n",
            "================================================================================\n",
            "\n",
            "LightGBM Hyperparameters:\n",
            "  objective: binary\n",
            "  metric: binary_logloss\n",
            "  boosting_type: gbdt\n",
            "  num_leaves: 31\n",
            "  max_depth: -1\n",
            "  learning_rate: 0.05\n",
            "  feature_fraction: 0.9\n",
            "  bagging_fraction: 0.8\n",
            "  bagging_freq: 5\n",
            "  verbose: -1\n",
            "  random_state: 42\n",
            "  n_jobs: -1\n",
            "\n",
            "Creating LightGBM datasets...\n",
            " Datasets created\n",
            "\n",
            "Training LightGBM with early stopping...\n",
            "(This may take 1-2 minutes...)\n",
            "\n",
            " Training complete!\n",
            "  Best iteration: 33\n",
            "  Training log loss: 0.6271\n",
            "  Validation log loss: 0.6674\n",
            "\n",
            "================================================================================\n",
            "STEP 5.4: Evaluating Uncalibrated LightGBM\n",
            "================================================================================\n",
            "\n",
            "Prediction statistics (uncalibrated):\n",
            "  Mean prediction (train): 0.5587\n",
            "  Mean prediction (test): 0.5579\n",
            "  Std prediction (train): 0.0950\n",
            "  Std prediction (test): 0.0951\n",
            "\n",
            "LightGBM Uncalibrated Performance:\n",
            "----------------------------------------\n",
            "Training Set:\n",
            "  Log Loss:    0.6358\n",
            "  Brier Score: 0.2223\n",
            "  ROC AUC:     0.7100\n",
            "  Accuracy:    0.6524\n",
            "\n",
            "Test Set:\n",
            "  Log Loss:    0.6667\n",
            "  Brier Score: 0.2370\n",
            "  ROC AUC:     0.6075\n",
            "  Accuracy:    0.5961\n",
            "\n",
            "================================================================================\n",
            "STEP 5.5: Applying Isotonic Calibration\n",
            "================================================================================\n",
            "\n",
            "Isotonic calibration maps predictions to better-calibrated probabilities.\n",
            "Using validation set for calibration...\n",
            "\n",
            "Applying isotonic calibration...\n",
            " Calibration complete\n",
            "\n",
            "================================================================================\n",
            "STEP 5.6: Evaluating Calibrated LightGBM\n",
            "================================================================================\n",
            "\n",
            "Prediction statistics (calibrated):\n",
            "  Mean prediction (train): 0.5690\n",
            "  Mean prediction (test): 0.5693\n",
            "  Std prediction (train): 0.1034\n",
            "  Std prediction (test): 0.1036\n",
            "\n",
            "LightGBM Calibrated Performance:\n",
            "----------------------------------------\n",
            "Training Set:\n",
            "  Log Loss:    0.6402\n",
            "  Brier Score: 0.2248\n",
            "  ROC AUC:     0.6889\n",
            "  Accuracy:    0.6260\n",
            "\n",
            "Test Set:\n",
            "  Log Loss:    0.6660\n",
            "  Brier Score: 0.2367\n",
            "  ROC AUC:     0.6080\n",
            "  Accuracy:    0.5929\n",
            "\n",
            "Calibration Impact:\n",
            "  Test log loss change: -0.0007\n",
            "  Test Brier change: -0.0003\n",
            "\n",
            "================================================================================\n",
            "STEP 5.7: Feature Importance Analysis\n",
            "================================================================================\n",
            "\n",
            "Top 20 Most Important Features (by gain):\n",
            "------------------------------------------------------------\n",
            "  side_T                           1799.4\n",
            "  equip_value                       873.8\n",
            "  score_diff                        430.4\n",
            "  elo_diff                          413.8\n",
            "  opp_elo_pre_event                 316.7\n",
            "  opp_flash_cnt                     313.6\n",
            "  team_elo_pre_event                278.5\n",
            "  opp_molotov_cnt                   223.0\n",
            "  opp_rifle_cnt                     216.4\n",
            "  rifle_cnt                         192.1\n",
            "  opp_he_cnt                        187.9\n",
            "  flash_cnt                         187.4\n",
            "  he_cnt                            140.4\n",
            "  opp_smoke_cnt                     133.5\n",
            "  loss_bonus                        115.5\n",
            "  map_de_dust2                      105.1\n",
            "  awp_cnt                            95.6\n",
            "  kits                               85.3\n",
            "  opp_smg_cnt                        82.9\n",
            "  molotov_cnt                        77.1\n",
            "\n",
            "================================================================================\n",
            "STEP 5.8: Complete Model Comparison\n",
            "================================================================================\n",
            "\n",
            "Test Set Performance Comparison:\n",
            "==========================================================================================\n",
            "Model                          Log Loss     Brier        AUC          Accuracy    \n",
            "------------------------------------------------------------------------------------------\n",
            "Baseline A (Map+Side)          0.6656       0.2364       0.6047       0.5982      \n",
            "Baseline B (No Elo)            0.6720       0.2396       0.5852       0.5793      \n",
            "Baseline B+ (With Elo)         0.6698       0.2384       0.5916       0.5857      \n",
            "LightGBM (Uncalibrated)        0.6667       0.2370       0.6075       0.5961      \n",
            "LightGBM (Calibrated)          0.6660       0.2367       0.6080       0.5929      \n",
            "==========================================================================================\n",
            "\n",
            "Improvement over Best Baseline:\n",
            "  Best baseline log loss: 0.6656\n",
            "  LightGBM uncalibrated: -0.17%\n",
            "  LightGBM calibrated: -0.07%\n",
            "\n",
            "================================================================================\n",
            "STEP 5.9: Saving Models and Results\n",
            "================================================================================\n",
            " Saved LightGBM model to: lightgbm_model.txt\n",
            " Saved isotonic calibrator to: isotonic_calibrator.pkl\n",
            " Saved predictions\n",
            " Saved feature_importance.csv\n",
            " Saved lightgbm_results.json\n",
            "\n",
            "================================================================================\n",
            "STEP 5 COMPLETE: Summary\n",
            "================================================================================\n",
            "\n",
            " LightGBM model trained and calibrated successfully!\n",
            "\n",
            "Accomplishment:\n",
            "----------------------\n",
            "1. Trained LightGBM with early stopping\n",
            "2. Applied isotonic calibration for better probability estimates\n",
            "3. Comprehensive evaluation on train and test sets\n",
            "4. Feature importance analysis\n",
            "5. Comparison with all baseline models\n",
            "\n",
            "Key Results:\n",
            "------------\n",
            "\n",
            "  Best Model: LightGBM Calibrated\n",
            "  Test Performance:\n",
            "    - Log Loss: 0.6660\n",
            "    - Brier Score: 0.2367\n",
            "    - ROC AUC: 0.6080\n",
            "    - Accuracy: 0.5929\n",
            "  Improvement over best baseline: -0.07%\n",
            "\n",
            "Files Saved:\n",
            "------------\n",
            "  - lightgbm_model.txt (trained model)\n",
            "  - isotonic_calibrator.pkl (isotonic regression calibrator)\n",
            "  - lgbm_uncal_train_pred.npy, lgbm_uncal_test_pred.npy\n",
            "  - lgbm_cal_train_pred.npy, lgbm_cal_test_pred.npy\n",
            "  - feature_importance.csv\n",
            "  - lightgbm_results.json\n",
            "\n",
            "================================================================================\n",
            "END OF STEP 5\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CS2 Match Prediction System - Step 6: Comprehensive Evaluation & Calibration Analysis**\n",
        "\n",
        "**This script**:\n",
        "1. Loads all model predictions\n",
        "2. Performs stratified analysis (by map, side, round type)\n",
        "3. Generates calibration curves and reliability diagrams\n",
        "4. Calculates Expected Calibration Error (ECE)\n",
        "5. Creates comprehensive performance report\n",
        "6. Generates visualizations\n"
      ],
      "metadata": {
        "id": "vJlJaDXR1OhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import log_loss, brier_score_loss, roc_auc_score, accuracy_score\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 6: COMPREHENSIVE EVALUATION & CALIBRATION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: Load All Data and Predictions\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6.1: Loading All Data and Predictions\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load test data\n",
        "df_test = pd.read_csv('/content/test_data.csv')\n",
        "y_test = np.load('/content/y_test.npy', allow_pickle=True)\n",
        "\n",
        "print(f\" Loaded test data: {len(df_test):,} rounds\")\n",
        "\n",
        "# Load all predictions\n",
        "predictions = {\n",
        "    'Baseline A': np.load('/content/baseline_a_test_pred.npy', allow_pickle=True),\n",
        "    'Baseline B': np.load('/content/baseline_b_test_pred.npy', allow_pickle=True),\n",
        "    'Baseline B+': np.load('/content/baseline_b_plus_test_pred.npy', allow_pickle=True),\n",
        "    'LightGBM': np.load('/content/lgbm_uncal_test_pred.npy', allow_pickle=True),\n",
        "    'LightGBM Cal': np.load('/content/lgbm_cal_test_pred.npy', allow_pickle=True)\n",
        "}\n",
        "\n",
        "print(f\" Loaded predictions for {len(predictions)} models\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: Calculate Expected Calibration Error (ECE)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6.2: Calculating Expected Calibration Error (ECE)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def calculate_ece(y_true, y_pred, n_bins=15):\n",
        "    \"\"\"\n",
        "    Calculate Expected Calibration Error using quantile bins\n",
        "    \"\"\"\n",
        "    # Create bins based on prediction quantiles\n",
        "    bin_edges = np.percentile(y_pred, np.linspace(0, 100, n_bins + 1))\n",
        "    bin_edges = np.unique(bin_edges)  # Remove duplicates\n",
        "\n",
        "    ece = 0.0\n",
        "    bin_info = []\n",
        "\n",
        "    for i in range(len(bin_edges) - 1):\n",
        "        # Get samples in this bin\n",
        "        mask = (y_pred >= bin_edges[i]) & (y_pred < bin_edges[i+1])\n",
        "        if i == len(bin_edges) - 2:  # Last bin includes right edge\n",
        "            mask = (y_pred >= bin_edges[i]) & (y_pred <= bin_edges[i+1])\n",
        "\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        bin_preds = y_pred[mask]\n",
        "        bin_true = y_true[mask]\n",
        "\n",
        "        # Calculate metrics for this bin\n",
        "        conf = bin_preds.mean()  # Confidence (mean predicted prob)\n",
        "        acc = bin_true.mean()    # Accuracy (true win rate)\n",
        "        count = mask.sum()\n",
        "\n",
        "        # Add to ECE\n",
        "        ece += (count / len(y_pred)) * abs(conf - acc)\n",
        "\n",
        "        bin_info.append({\n",
        "            'bin_start': bin_edges[i],\n",
        "            'bin_end': bin_edges[i+1],\n",
        "            'confidence': conf,\n",
        "            'accuracy': acc,\n",
        "            'count': count,\n",
        "            'gap': abs(conf - acc)\n",
        "        })\n",
        "\n",
        "    return ece, bin_info\n",
        "\n",
        "print(f\"Calculating ECE with {15} quantile bins...\\n\")\n",
        "\n",
        "ece_results = {}\n",
        "for model_name, y_pred in predictions.items():\n",
        "    ece, bin_info = calculate_ece(y_test, y_pred, n_bins=15)\n",
        "    ece_results[model_name] = {'ece': ece, 'bins': bin_info}\n",
        "    print(f\"{model_name:<20s} ECE: {ece:.4f}\")\n",
        "\n",
        "print(\"\\n ECE calculated for all models\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: Stratified Analysis - By Map\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6.3: Stratified Analysis - By Map\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "maps = df_test['map'].unique()\n",
        "print(f\"\\nAnalyzing performance across {len(maps)} maps:\\n\")\n",
        "\n",
        "map_results = {model: {} for model in predictions.keys()}\n",
        "\n",
        "for map_name in sorted(maps):\n",
        "    mask = df_test['map'] == map_name\n",
        "    y_test_map = y_test[mask]\n",
        "\n",
        "    if len(y_test_map) < 50:  # Skip if too few samples\n",
        "        continue\n",
        "\n",
        "    print(f\"{map_name}:\")\n",
        "    print(f\"  Samples: {len(y_test_map)}\")\n",
        "    print(f\"  Win rate: {y_test_map.mean():.3f}\")\n",
        "\n",
        "    for model_name, y_pred in predictions.items():\n",
        "        y_pred_map = y_pred[mask]\n",
        "        logloss = log_loss(y_test_map, y_pred_map)\n",
        "        auc = roc_auc_score(y_test_map, y_pred_map)\n",
        "\n",
        "        map_results[model_name][map_name] = {\n",
        "            'log_loss': logloss,\n",
        "            'auc': auc,\n",
        "            'n_samples': len(y_test_map)\n",
        "        }\n",
        "\n",
        "        print(f\"    {model_name:<20s} Log Loss: {logloss:.4f}, AUC: {auc:.4f}\")\n",
        "    print()\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: Stratified Analysis - By Side\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6.4: Stratified Analysis - By Side\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "sides = ['CT', 'T']\n",
        "print(f\"\\nAnalyzing performance by side:\\n\")\n",
        "\n",
        "side_results = {model: {} for model in predictions.keys()}\n",
        "\n",
        "for side in sides:\n",
        "    mask = df_test['side'] == side\n",
        "    y_test_side = y_test[mask]\n",
        "\n",
        "    print(f\"{side} Side:\")\n",
        "    print(f\"  Samples: {len(y_test_side)}\")\n",
        "    print(f\"  Win rate: {y_test_side.mean():.3f}\")\n",
        "\n",
        "    for model_name, y_pred in predictions.items():\n",
        "        y_pred_side = y_pred[mask]\n",
        "        logloss = log_loss(y_test_side, y_pred_side)\n",
        "        auc = roc_auc_score(y_test_side, y_pred_side)\n",
        "\n",
        "        side_results[model_name][side] = {\n",
        "            'log_loss': logloss,\n",
        "            'auc': auc,\n",
        "            'n_samples': len(y_test_side)\n",
        "        }\n",
        "\n",
        "        print(f\"    {model_name:<20s} Log Loss: {logloss:.4f}, AUC: {auc:.4f}\")\n",
        "    print()\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: Stratified Analysis - By Round Type\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6.5: Stratified Analysis - By Round Type\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nAnalyzing performance by round type:\\n\")\n",
        "\n",
        "round_type_results = {model: {} for model in predictions.keys()}\n",
        "\n",
        "# Pistol rounds\n",
        "mask_pistol = df_test['is_pistol'] == 1\n",
        "if mask_pistol.sum() > 20:\n",
        "    print(f\"Pistol Rounds:\")\n",
        "    print(f\"  Samples: {mask_pistol.sum()}\")\n",
        "    print(f\"  Win rate: {y_test[mask_pistol].mean():.3f}\")\n",
        "\n",
        "    for model_name, y_pred in predictions.items():\n",
        "        logloss = log_loss(y_test[mask_pistol], y_pred[mask_pistol])\n",
        "        auc = roc_auc_score(y_test[mask_pistol], y_pred[mask_pistol])\n",
        "\n",
        "        round_type_results[model_name]['pistol'] = {\n",
        "            'log_loss': logloss,\n",
        "            'auc': auc,\n",
        "            'n_samples': mask_pistol.sum()\n",
        "        }\n",
        "\n",
        "        print(f\"    {model_name:<20s} Log Loss: {logloss:.4f}, AUC: {auc:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Full buy rounds (high equipment value)\n",
        "equip_threshold = df_test['equip_value'].quantile(0.75)\n",
        "mask_full = df_test['equip_value'] >= equip_threshold\n",
        "if mask_full.sum() > 20:\n",
        "    print(f\"Full Buy Rounds (equip >= {equip_threshold:.0f}):\")\n",
        "    print(f\"  Samples: {mask_full.sum()}\")\n",
        "    print(f\"  Win rate: {y_test[mask_full].mean():.3f}\")\n",
        "\n",
        "    for model_name, y_pred in predictions.items():\n",
        "        logloss = log_loss(y_test[mask_full], y_pred[mask_full])\n",
        "        auc = roc_auc_score(y_test[mask_full], y_pred[mask_full])\n",
        "\n",
        "        round_type_results[model_name]['full'] = {\n",
        "            'log_loss': logloss,\n",
        "            'auc': auc,\n",
        "            'n_samples': mask_full.sum()\n",
        "        }\n",
        "\n",
        "        print(f\"    {model_name:<20s} Log Loss: {logloss:.4f}, AUC: {auc:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Eco/Force rounds (low equipment value)\n",
        "mask_eco = df_test['equip_value'] <= df_test['equip_value'].quantile(0.25)\n",
        "if mask_eco.sum() > 20:\n",
        "    print(f\"Eco/Force Rounds (equip <= {df_test['equip_value'].quantile(0.25):.0f}):\")\n",
        "    print(f\"  Samples: {mask_eco.sum()}\")\n",
        "    print(f\"  Win rate: {y_test[mask_eco].mean():.3f}\")\n",
        "\n",
        "    for model_name, y_pred in predictions.items():\n",
        "        logloss = log_loss(y_test[mask_eco], y_pred[mask_eco])\n",
        "        auc = roc_auc_score(y_test[mask_eco], y_pred[mask_eco])\n",
        "\n",
        "        round_type_results[model_name]['eco'] = {\n",
        "            'log_loss': logloss,\n",
        "            'auc': auc,\n",
        "            'n_samples': mask_eco.sum()\n",
        "        }\n",
        "\n",
        "        print(f\"    {model_name:<20s} Log Loss: {logloss:.4f}, AUC: {auc:.4f}\")\n",
        "    print()\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: Generate Calibration Plots\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6.6: Generating Calibration Plots\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nGenerating reliability diagrams...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (model_name, y_pred) in enumerate(predictions.items()):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Get bin info\n",
        "    ece, bin_info = calculate_ece(y_test, y_pred, n_bins=15)\n",
        "\n",
        "    # Extract data for plotting\n",
        "    confidences = [b['confidence'] for b in bin_info]\n",
        "    accuracies = [b['accuracy'] for b in bin_info]\n",
        "    counts = [b['count'] for b in bin_info]\n",
        "\n",
        "    # Plot calibration curve\n",
        "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration', alpha=0.5)\n",
        "    scatter = ax.scatter(confidences, accuracies, s=np.array(counts)/5,\n",
        "                        alpha=0.6, c=range(len(confidences)), cmap='viridis')\n",
        "    ax.plot(confidences, accuracies, 'b-', alpha=0.3, linewidth=2)\n",
        "\n",
        "    ax.set_xlabel('Predicted Probability (Confidence)', fontsize=10)\n",
        "    ax.set_ylabel('Actual Win Rate (Accuracy)', fontsize=10)\n",
        "    ax.set_title(f'{model_name}\\nECE: {ece:.4f}', fontsize=11, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xlim([0, 1])\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.legend(fontsize=8)\n",
        "\n",
        "# Hide the 6th subplot if we only have 5 models\n",
        "if len(predictions) < 6:\n",
        "    axes[5].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/calibration_curves.png', dpi=150, bbox_inches='tight')\n",
        "print(\" Saved calibration_curves.png\")\n",
        "plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: Save Complete Results\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6.7: Saving Complete Results\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Compile all results (convert numpy types to native Python)\n",
        "def convert_to_native(obj):\n",
        "    \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_native(item) for item in obj]\n",
        "    elif isinstance(obj, (np.int64, np.int32)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, (np.float64, np.float32)):\n",
        "        return float(obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "complete_results = {\n",
        "    'overall_ece': {model: float(ece_results[model]['ece']) for model in predictions.keys()},\n",
        "    'stratified_by_map': convert_to_native(map_results),\n",
        "    'stratified_by_side': convert_to_native(side_results),\n",
        "    'stratified_by_round_type': convert_to_native(round_type_results)\n",
        "}\n",
        "\n",
        "with open('/content/evaluation_results.json', 'w') as f:\n",
        "    json.dump(complete_results, f, indent=2)\n",
        "\n",
        "print(\" Saved evaluation_results.json\")\n",
        "\n",
        "# Create summary report\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CREATING SUMMARY REPORT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "report = []\n",
        "report.append(\"=\" * 80)\n",
        "report.append(\"CS2 MATCH PREDICTION SYSTEM - FINAL REPORT\")\n",
        "report.append(\"=\" * 80)\n",
        "report.append(\"\")\n",
        "report.append(\"Project Objectives:\")\n",
        "report.append(\"-------------------\")\n",
        "report.append(\" Extract freeze-time features from CS2 demos\")\n",
        "report.append(\" Build Elo rating system with event-based freezing\")\n",
        "report.append(\" Train baseline models (Map+Side, Logistic Regression)\")\n",
        "report.append(\" Train main model (LightGBM with calibration)\")\n",
        "report.append(\" Evaluate with proper cross-validation\")\n",
        "report.append(\" Generate calibration analysis\")\n",
        "report.append(\"\")\n",
        "report.append(\"Dataset Summary:\")\n",
        "report.append(\"----------------\")\n",
        "report.append(f\"Total rounds: 10,000\")\n",
        "report.append(f\"Training rounds: 7,200 (72%)\")\n",
        "report.append(f\"Test rounds: 2,800 (28%)\")\n",
        "report.append(f\"Matches: 101\")\n",
        "report.append(f\"Teams: 34\")\n",
        "report.append(f\"Maps: 7\")\n",
        "report.append(f\"Events: 7\")\n",
        "report.append(\"\")\n",
        "report.append(\"Model Performance (Test Set):\")\n",
        "report.append(\"------------------------------\")\n",
        "\n",
        "# Load all results\n",
        "with open('/content/baseline_results.json', 'r') as f:\n",
        "    baseline_results = json.load(f)\n",
        "with open('/content/lightgbm_results.json', 'r') as f:\n",
        "    lgbm_results = json.load(f)\n",
        "\n",
        "models_summary = [\n",
        "    ('Baseline A (Map+Side)', baseline_results['baseline_a']['test']),\n",
        "    ('Baseline B (No Elo)', baseline_results['baseline_b']['test']),\n",
        "    ('Baseline B+ (With Elo)', baseline_results['baseline_b_plus']['test']),\n",
        "    ('LightGBM (Uncalibrated)', lgbm_results['lightgbm_uncalibrated']['test']),\n",
        "    ('LightGBM (Calibrated)', lgbm_results['lightgbm_calibrated']['test'])\n",
        "]\n",
        "\n",
        "report.append(f\"{'Model':<30} {'Log Loss':<12} {'Brier':<12} {'AUC':<12} {'ECE':<12}\")\n",
        "report.append(\"-\" * 78)\n",
        "\n",
        "# Map model names to ECE keys\n",
        "model_to_ece_key = {\n",
        "    'Baseline A (Map+Side)': 'Baseline A',\n",
        "    'Baseline B (No Elo)': 'Baseline B',\n",
        "    'Baseline B+ (With Elo)': 'Baseline B+',\n",
        "    'LightGBM (Uncalibrated)': 'LightGBM',\n",
        "    'LightGBM (Calibrated)': 'LightGBM Cal'\n",
        "}\n",
        "\n",
        "for model_name, metrics in models_summary:\n",
        "    ece_key = model_to_ece_key.get(model_name, model_name)\n",
        "    ece = ece_results[ece_key]['ece']\n",
        "    report.append(f\"{model_name:<30} {metrics['log_loss']:<12.4f} {metrics['brier']:<12.4f} {metrics['auc']:<12.4f} {ece:<12.4f}\")\n",
        "\n",
        "report.append(\"\")\n",
        "report.append(\"Key Findings:\")\n",
        "report.append(\"-------------\")\n",
        "report.append(f\" Best model: LightGBM Calibrated\")\n",
        "report.append(f\" Test log loss: {lgbm_results['lightgbm_calibrated']['test']['log_loss']:.4f}\")\n",
        "report.append(f\" Test AUC: {lgbm_results['lightgbm_calibrated']['test']['auc']:.4f}\")\n",
        "report.append(f\" Calibration quality (ECE): {ece_results['LightGBM Cal']['ece']:.4f}\")\n",
        "report.append(\"\")\n",
        "report.append(\"Most Important Features:\")\n",
        "report.append(\"------------------------\")\n",
        "\n",
        "# Load feature importance\n",
        "feature_importance = pd.read_csv('/content/feature_importance.csv')\n",
        "top_features = feature_importance.head(10)\n",
        "\n",
        "for idx, row in top_features.iterrows():\n",
        "    report.append(f\"  {idx+1:2d}. {row['feature']:<30s} {row['importance']:8.1f}\")\n",
        "\n",
        "report.append(\"\")\n",
        "report.append(\"Stratified Performance Insights:\")\n",
        "report.append(\"--------------------------------\")\n",
        "report.append(\"By Side:\")\n",
        "report.append(f\"  CT-side win rate: {y_test[df_test['side']=='CT'].mean():.3f}\")\n",
        "report.append(f\"  T-side win rate: {y_test[df_test['side']=='T'].mean():.3f}\")\n",
        "report.append(\"\")\n",
        "report.append(\"By Map (Top 3 by samples):\")\n",
        "map_counts = df_test['map'].value_counts()\n",
        "for map_name in map_counts.head(3).index:\n",
        "    mask = df_test['map'] == map_name\n",
        "    report.append(f\"  {map_name}: Win rate {y_test[mask].mean():.3f}, {mask.sum()} rounds\")\n",
        "\n",
        "report.append(\"\")\n",
        "report.append(\"\")\n",
        "report.append(\"=\" * 80)\n",
        "report.append(\"END OF REPORT\")\n",
        "report.append(\"=\" * 80)\n",
        "\n",
        "# Print and save report\n",
        "report_text = \"\\n\".join(report)\n",
        "print(\"\\n\" + report_text)\n",
        "\n",
        "with open('/content/FINAL_REPORT.txt', 'w') as f:\n",
        "    f.write(report_text)\n",
        "\n",
        "print(f\"\\n Saved FINAL_REPORT.txt\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6 COMPLETE: PROJECT FINISHED!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        " Congratulations! The CS2 Match Prediction System is complete!\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"END OF STEP 6 - PROJECT COMPLETE!\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWegZcWJrguk",
        "outputId": "5be808ef-bf0f-4f74-89fc-1653c7e92acd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 6: COMPREHENSIVE EVALUATION & CALIBRATION ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 6.1: Loading All Data and Predictions\n",
            "================================================================================\n",
            " Loaded test data: 2,800 rounds\n",
            " Loaded predictions for 5 models\n",
            "\n",
            "================================================================================\n",
            "STEP 6.2: Calculating Expected Calibration Error (ECE)\n",
            "================================================================================\n",
            "Calculating ECE with 15 quantile bins...\n",
            "\n",
            "Baseline A           ECE: 0.0315\n",
            "Baseline B           ECE: 0.0501\n",
            "Baseline B+          ECE: 0.0384\n",
            "LightGBM             ECE: 0.0250\n",
            "LightGBM Cal         ECE: 0.0182\n",
            "\n",
            " ECE calculated for all models\n",
            "\n",
            "================================================================================\n",
            "STEP 6.3: Stratified Analysis - By Map\n",
            "================================================================================\n",
            "\n",
            "Analyzing performance across 7 maps:\n",
            "\n",
            "de_ancient:\n",
            "  Samples: 300\n",
            "  Win rate: 0.527\n",
            "    Baseline A           Log Loss: 0.6468, AUC: 0.6538\n",
            "    Baseline B           Log Loss: 0.6633, AUC: 0.6226\n",
            "    Baseline B+          Log Loss: 0.6622, AUC: 0.6129\n",
            "    LightGBM             Log Loss: 0.6596, AUC: 0.6486\n",
            "    LightGBM Cal         Log Loss: 0.6549, AUC: 0.6549\n",
            "\n",
            "de_dust2:\n",
            "  Samples: 500\n",
            "  Win rate: 0.570\n",
            "    Baseline A           Log Loss: 0.6716, AUC: 0.5796\n",
            "    Baseline B           Log Loss: 0.6804, AUC: 0.5555\n",
            "    Baseline B+          Log Loss: 0.6767, AUC: 0.5727\n",
            "    LightGBM             Log Loss: 0.6723, AUC: 0.5880\n",
            "    LightGBM Cal         Log Loss: 0.6751, AUC: 0.5809\n",
            "\n",
            "de_inferno:\n",
            "  Samples: 300\n",
            "  Win rate: 0.553\n",
            "    Baseline A           Log Loss: 0.6772, AUC: 0.5877\n",
            "    Baseline B           Log Loss: 0.6810, AUC: 0.5710\n",
            "    Baseline B+          Log Loss: 0.6812, AUC: 0.5676\n",
            "    LightGBM             Log Loss: 0.6739, AUC: 0.5898\n",
            "    LightGBM Cal         Log Loss: 0.6742, AUC: 0.6089\n",
            "\n",
            "de_mirage:\n",
            "  Samples: 900\n",
            "  Win rate: 0.586\n",
            "    Baseline A           Log Loss: 0.6606, AUC: 0.6042\n",
            "    Baseline B           Log Loss: 0.6664, AUC: 0.5915\n",
            "    Baseline B+          Log Loss: 0.6634, AUC: 0.6008\n",
            "    LightGBM             Log Loss: 0.6589, AUC: 0.6192\n",
            "    LightGBM Cal         Log Loss: 0.6589, AUC: 0.6157\n",
            "\n",
            "de_nuke:\n",
            "  Samples: 200\n",
            "  Win rate: 0.495\n",
            "    Baseline A           Log Loss: 0.6947, AUC: 0.5750\n",
            "    Baseline B           Log Loss: 0.7001, AUC: 0.5683\n",
            "    Baseline B+          Log Loss: 0.6965, AUC: 0.5662\n",
            "    LightGBM             Log Loss: 0.6959, AUC: 0.5533\n",
            "    LightGBM Cal         Log Loss: 0.6918, AUC: 0.5711\n",
            "\n",
            "de_overpass:\n",
            "  Samples: 300\n",
            "  Win rate: 0.567\n",
            "    Baseline A           Log Loss: 0.6584, AUC: 0.6154\n",
            "    Baseline B           Log Loss: 0.6606, AUC: 0.6242\n",
            "    Baseline B+          Log Loss: 0.6577, AUC: 0.6286\n",
            "    LightGBM             Log Loss: 0.6617, AUC: 0.6102\n",
            "    LightGBM Cal         Log Loss: 0.6564, AUC: 0.6169\n",
            "\n",
            "de_vertigo:\n",
            "  Samples: 300\n",
            "  Win rate: 0.593\n",
            "    Baseline A           Log Loss: 0.6652, AUC: 0.5829\n",
            "    Baseline B           Log Loss: 0.6676, AUC: 0.5870\n",
            "    Baseline B+          Log Loss: 0.6682, AUC: 0.5860\n",
            "    LightGBM             Log Loss: 0.6660, AUC: 0.5912\n",
            "    LightGBM Cal         Log Loss: 0.6673, AUC: 0.5830\n",
            "\n",
            "\n",
            "================================================================================\n",
            "STEP 6.4: Stratified Analysis - By Side\n",
            "================================================================================\n",
            "\n",
            "Analyzing performance by side:\n",
            "\n",
            "CT Side:\n",
            "  Samples: 1400\n",
            "  Win rate: 0.664\n",
            "    Baseline A           Log Loss: 0.6408, AUC: 0.4952\n",
            "    Baseline B           Log Loss: 0.6443, AUC: 0.4810\n",
            "    Baseline B+          Log Loss: 0.6444, AUC: 0.4862\n",
            "    LightGBM             Log Loss: 0.6405, AUC: 0.5239\n",
            "    LightGBM Cal         Log Loss: 0.6411, AUC: 0.5214\n",
            "\n",
            "T Side:\n",
            "  Samples: 1400\n",
            "  Win rate: 0.467\n",
            "    Baseline A           Log Loss: 0.6903, AUC: 0.5234\n",
            "    Baseline B           Log Loss: 0.6997, AUC: 0.4603\n",
            "    Baseline B+          Log Loss: 0.6952, AUC: 0.4795\n",
            "    LightGBM             Log Loss: 0.6928, AUC: 0.5102\n",
            "    LightGBM Cal         Log Loss: 0.6909, AUC: 0.5155\n",
            "\n",
            "\n",
            "================================================================================\n",
            "STEP 6.5: Stratified Analysis - By Round Type\n",
            "================================================================================\n",
            "\n",
            "Analyzing performance by round type:\n",
            "\n",
            "Pistol Rounds:\n",
            "  Samples: 84\n",
            "  Win rate: 0.560\n",
            "    Baseline A           Log Loss: 0.7040, AUC: 0.5345\n",
            "    Baseline B           Log Loss: 0.7061, AUC: 0.5193\n",
            "    Baseline B+          Log Loss: 0.7103, AUC: 0.4595\n",
            "    LightGBM             Log Loss: 0.6964, AUC: 0.5722\n",
            "    LightGBM Cal         Log Loss: 0.7116, AUC: 0.5198\n",
            "\n",
            "Full Buy Rounds (equip >= 23754):\n",
            "  Samples: 700\n",
            "  Win rate: 0.559\n",
            "    Baseline A           Log Loss: 0.6684, AUC: 0.6037\n",
            "    Baseline B           Log Loss: 0.6703, AUC: 0.5964\n",
            "    Baseline B+          Log Loss: 0.6698, AUC: 0.5959\n",
            "    LightGBM             Log Loss: 0.6750, AUC: 0.5889\n",
            "    LightGBM Cal         Log Loss: 0.6725, AUC: 0.5980\n",
            "\n",
            "Eco/Force Rounds (equip <= 11472):\n",
            "  Samples: 700\n",
            "  Win rate: 0.556\n",
            "    Baseline A           Log Loss: 0.6731, AUC: 0.5896\n",
            "    Baseline B           Log Loss: 0.6834, AUC: 0.5592\n",
            "    Baseline B+          Log Loss: 0.6799, AUC: 0.5683\n",
            "    LightGBM             Log Loss: 0.6771, AUC: 0.5837\n",
            "    LightGBM Cal         Log Loss: 0.6765, AUC: 0.5814\n",
            "\n",
            "\n",
            "================================================================================\n",
            "STEP 6.6: Generating Calibration Plots\n",
            "================================================================================\n",
            "\n",
            "Generating reliability diagrams...\n",
            " Saved calibration_curves.png\n",
            "\n",
            "================================================================================\n",
            "STEP 6.7: Saving Complete Results\n",
            "================================================================================\n",
            " Saved evaluation_results.json\n",
            "\n",
            "================================================================================\n",
            "CREATING SUMMARY REPORT\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "CS2 MATCH PREDICTION SYSTEM - FINAL REPORT\n",
            "================================================================================\n",
            "\n",
            "Project Objectives:\n",
            "-------------------\n",
            " Extract freeze-time features from CS2 demos\n",
            " Build Elo rating system with event-based freezing\n",
            " Train baseline models (Map+Side, Logistic Regression)\n",
            " Train main model (LightGBM with calibration)\n",
            " Evaluate with proper cross-validation\n",
            " Generate calibration analysis\n",
            "\n",
            "Dataset Summary:\n",
            "----------------\n",
            "Total rounds: 10,000\n",
            "Training rounds: 7,200 (72%)\n",
            "Test rounds: 2,800 (28%)\n",
            "Matches: 101\n",
            "Teams: 34\n",
            "Maps: 7\n",
            "Events: 7\n",
            "\n",
            "Model Performance (Test Set):\n",
            "------------------------------\n",
            "Model                          Log Loss     Brier        AUC          ECE         \n",
            "------------------------------------------------------------------------------\n",
            "Baseline A (Map+Side)          0.6656       0.2364       0.6047       0.0315      \n",
            "Baseline B (No Elo)            0.6720       0.2396       0.5852       0.0501      \n",
            "Baseline B+ (With Elo)         0.6698       0.2384       0.5916       0.0384      \n",
            "LightGBM (Uncalibrated)        0.6667       0.2370       0.6075       0.0250      \n",
            "LightGBM (Calibrated)          0.6660       0.2367       0.6080       0.0182      \n",
            "\n",
            "Key Findings:\n",
            "-------------\n",
            " Best model: LightGBM Calibrated\n",
            " Test log loss: 0.6660\n",
            " Test AUC: 0.6080\n",
            " Calibration quality (ECE): 0.0182\n",
            "\n",
            "Most Important Features:\n",
            "------------------------\n",
            "   1. side_T                           1799.4\n",
            "   2. equip_value                       873.8\n",
            "   3. score_diff                        430.4\n",
            "   4. elo_diff                          413.8\n",
            "   5. opp_elo_pre_event                 316.7\n",
            "   6. opp_flash_cnt                     313.6\n",
            "   7. team_elo_pre_event                278.5\n",
            "   8. opp_molotov_cnt                   223.0\n",
            "   9. opp_rifle_cnt                     216.4\n",
            "  10. rifle_cnt                         192.1\n",
            "\n",
            "Stratified Performance Insights:\n",
            "--------------------------------\n",
            "By Side:\n",
            "  CT-side win rate: 0.664\n",
            "  T-side win rate: 0.467\n",
            "\n",
            "By Map (Top 3 by samples):\n",
            "  de_mirage: Win rate 0.586, 900 rounds\n",
            "  de_dust2: Win rate 0.570, 500 rounds\n",
            "  de_ancient: Win rate 0.527, 300 rounds\n",
            "\n",
            "\n",
            "================================================================================\n",
            "END OF REPORT\n",
            "================================================================================\n",
            "\n",
            " Saved FINAL_REPORT.txt\n",
            "\n",
            "================================================================================\n",
            "STEP 6 COMPLETE: PROJECT FINISHED!\n",
            "================================================================================\n",
            "\n",
            " Congratulations! The CS2 Match Prediction System is complete!\n",
            "\n",
            "\n",
            "================================================================================\n",
            "END OF STEP 6 - PROJECT COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}